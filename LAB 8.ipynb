{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3698d1-880a-4c4d-be9a-e89d96d07651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Importar librerías y definir funciones base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cc9bc0-f6e4-4d60-ba7f-f8d6f9af28d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "%pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7c33f0-52c7-45a2-b49e-bc05babd962f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff737ad-c5a3-4d8e-a16e-c2e01074fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carga desde archivo y construcción dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943e93f2-79e5-4c1d-a016-d46d687b0d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "from itertools import chain\n",
    "\n",
    "directories = [\n",
    "    \"/Volumes/workspace/default/fallecidos_lesionados/\",\n",
    "    \"/Volumes/workspace/default/hechos_transito/\",\n",
    "    \"/Volumes/workspace/default/vehiculos_involucrados/\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    try:\n",
    "        files = [f.name for f in dbutils.fs.ls(directory)]\n",
    "        \n",
    "        for file in files:\n",
    "            lower_file = file.lower()\n",
    "            file_path = os.path.join(directory, file)\n",
    "            \n",
    "            # XLSX\n",
    "            if lower_file.endswith(\".xlsx\"):\n",
    "                csv_path = file_path.replace(\".xlsx\", \".csv\")\n",
    "                try:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    print(f\"Convertido XLSX → CSV: {file_path} → {csv_path}\")\n",
    "                    \n",
    "                    # Borrar original\n",
    "                    dbutils.fs.rm(file_path)\n",
    "                    print(f\"Borrado XLSX: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error con XLSX {file_path}: {e}\")\n",
    "            \n",
    "            # SAV\n",
    "            elif lower_file.endswith(\".sav\"):\n",
    "                csv_path = file_path.replace(\".sav\", \".csv\")\n",
    "                try:\n",
    "                    df, meta = pyreadstat.read_sav(file_path)\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    print(f\"Convertido SAV → CSV: {file_path} → {csv_path}\")\n",
    "                    \n",
    "                    # Borrar original\n",
    "                    dbutils.fs.rm(file_path)\n",
    "                    print(f\"Borrado SAV: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error con SAV {file_path}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar directorio {directory}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15a6b2c-1cc0-4f2d-a320-fd46e3280130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unir TODOS los CSV de una carpeta en un solo DataFrame (robusto a columnas distintas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b0b017-f7cf-4256-802e-4a2d6e7f924a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "DIR_HECHOS = \"/Volumes/workspace/default/hechos_transito\"\n",
    "DIR_VEHICULOS = \"/Volumes/workspace/default/vehiculos_involucrados\"\n",
    "DIR_FALLECIDOS = \"/Volumes/workspace/default/fallecidos_lesionados\"\n",
    "\n",
    "# Opción simple (si todos los CSV ya tienen mismas columnas):\n",
    "hechos_raw = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DIR_HECHOS}/*.csv\")\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "display(hechos_raw.limit(5))\n",
    "hechos_raw.printSchema()\n",
    "\n",
    "vehiculos_raw = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DIR_VEHICULOS}/*.csv\")\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "display(vehiculos_raw.limit(5))\n",
    "vehiculos_raw.printSchema()\n",
    "\n",
    "fallecidos_raw = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DIR_FALLECIDOS}/*.csv\")\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "display(fallecidos_raw.limit(5))\n",
    "fallecidos_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5adc32-2e64-422a-9178-59d98711176b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# 1) alias de columnas → nombre canónico\n",
    "ALIASES = {\n",
    "    \"num_hecho\":      [\"num_hecho\", \"num\", \"num_correl\", \"num_corre\", \"n_um_corre\", \"num_correlativo\", \"núm_corre\"],\n",
    "    \"anio\":           [\"anio\", \"ano_ocu\", \"año_ocu\", \"anio_ocu\", \"ano\", \"año\"],\n",
    "    \"mes\":            [\"mes\", \"mes_ocu\"],\n",
    "    \"dia\":            [\"dia\", \"día\", \"dia_ocu\", \"día_ocu\"],\n",
    "    \"dia_sem\":        [\"dia_sem_ocu\", \"día_sem_ocu\", \"dia_sem\", \"día_sem\"],\n",
    "    \"hora\":           [\"hora\", \"hora_ocu\"],\n",
    "    \"g_hora\":         [\"g_hora\", \"g_hora_5\"],\n",
    "    \"depto\":          [\"depto_ocu\", \"depto\"],\n",
    "    \"mupio\":          [\"mupio_ocu\", \"muni_ocu\", \"municipio\", \"mupio\"],\n",
    "    \"zona\":           [\"zona_ocu\", \"zona\"],\n",
    "    \"area\":           [\"areag_ocu\", \"area_ocu\", \"area\"],\n",
    "    \"tipo_accidente\": [\"tipo_eve\", \"tipo_evento\", \"tipo\", \"tipo_acc\", \"tipo_accidente\"],\n",
    "    \"causa_acc\":      [\"causa_acc\", \"causa\"],\n",
    "    \"sexo_pil\":       [\"sexo_pil\", \"sexo_piloto\", \"sexo\", \"sexo_per\"],\n",
    "    \"edad_pil\":       [\"edad_pil\", \"edad\", \"edad_piloto\", \"edad_per\"],\n",
    "    \"g_edad\":         [\"g_edad_2\", \"g_edad\", \"g_edad_80ymás\", \"g_edad_60ymás\", \"edad_quinquenales\"],\n",
    "    \"mayor_menor\":    [\"mayor_menor\"],\n",
    "    \"tipo_veh\":       [\"tipo_veh\", \"tipo_vehiculo\"],\n",
    "    \"marca_veh\":      [\"marca_veh\", \"marca\"],\n",
    "    \"color_veh\":      [\"color_veh\", \"color\"],\n",
    "    \"modelo_veh\":     [\"modelo_veh\", \"modelo\"],\n",
    "    \"g_modelo_veh\":   [\"g_modelo_veh\"],\n",
    "    \"estado_pil\":     [\"estado_pil\", \"estado_piloto\", \"estado\", \"estado_con\", \"fall_les\"],\n",
    "    \"intencionalidad\":[\"int_o_noint\"],\n",
    "    \"source_file\":    [\"source_file\", \"_source_file\"]\n",
    "}\n",
    "\n",
    "def first_present(colnames, candidates):\n",
    "    cols_lower = {c.lower(): c for c in colnames}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def canonize(df):\n",
    "    # construir mapeo existente -> canon\n",
    "    mapping = {}\n",
    "    for canon, cands in ALIASES.items():\n",
    "        src = first_present(df.columns, cands)\n",
    "        if src and src != canon:\n",
    "            mapping[src] = canon\n",
    "\n",
    "    # aplicar renombres\n",
    "    for src, dst in mapping.items():\n",
    "        df = df.withColumnRenamed(src, dst)\n",
    "\n",
    "    return df\n",
    "\n",
    "hechos_std = canonize(hechos_raw)\n",
    "fallecidos_std = canonize(fallecidos_raw)\n",
    "vehiculos_std = canonize(vehiculos_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4fb1ec-49d9-4d76-b1df-59fe7a3e085e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from itertools import chain\n",
    "\n",
    "for df_name in [\"hechos_std\", \"fallecidos_std\", \"vehiculos_std\"]:\n",
    "    df = locals()[df_name]\n",
    "    if \"anio\" not in df.columns and \"source_file\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"anio\",\n",
    "            F.regexp_extract(\"source_file\", r\"(20\\d{2})\", 1).cast(\"int\")\n",
    "        )\n",
    "        locals()[df_name] = df\n",
    "\n",
    "\n",
    "CODIGOS_DEPARTAMENTOS = {\n",
    "    1:\"Guatemala\",2:\"El Progreso\",3:\"Sacatepéquez\",4:\"Chimaltenango\",5:\"Escuintla\",6:\"Santa Rosa\",\n",
    "    7:\"Sololá\",8:\"Totonicapán\",9:\"Quetzaltenango\",10:\"Suchitepéquez\",11:\"Retalhuleu\",12:\"San Marcos\",\n",
    "    13:\"Huehuetenango\",14:\"Quiché\",15:\"Baja Verapaz\",16:\"Alta Verapaz\",17:\"Petén\",18:\"Izabal\",\n",
    "    19:\"Zacapa\",20:\"Chiquimula\",21:\"Jalapa\",22:\"Jutiapa\"\n",
    "}\n",
    "EQUIVALENCIAS_DEPARTAMENTOS = {\n",
    "    \"Guatemala\":\"Guatemala\",\"Alta Verapaz\":\"Alta Verapaz\",\"Baja Verapaz\":\"Baja Verapaz\",\n",
    "    \"Chimaltenango\":\"Chimaltenango\",\"Chiquimula\":\"Chiquimula\",\"El Progreso\":\"El Progreso\",\n",
    "    \"Escuintla\":\"Escuintla\",\"Huehuetenango\":\"Huehuetenango\",\"Izabal\":\"Izabal\",\n",
    "    \"Jalapa\":\"Jalapa\",\"Jutiapa\":\"Jutiapa\",\"Peten\":\"Petén\",\"Pet?n\":\"Petén\",\"Petén\":\"Petén\",\n",
    "    \"Quiche\":\"Quiché\",\"Quich?\":\"Quiché\",\"Quiché\":\"Quiché\",\"Quetzaltenango\":\"Quetzaltenango\",\n",
    "    \"Retalhuleu\":\"Retalhuleu\",\"Sacatepequez\":\"Sacatepéquez\",\"Sacatep?quez\":\"Sacatepéquez\",\n",
    "    \"Sacatepéquez\":\"Sacatepéquez\",\"San Marcos\":\"San Marcos\",\"Santa Rosa\":\"Santa Rosa\",\n",
    "    \"Solola\":\"Sololá\",\"Solol?\":\"Sololá\",\"Sololá\":\"Sololá\",\"Suchitepequez\":\"Suchitepéquez\",\n",
    "    \"Suchitep?quez\":\"Suchitepéquez\",\"Suchitepéquez\":\"Suchitepéquez\",\"Totonicapan\":\"Totonicapán\",\n",
    "    \"Totonicap?n\":\"Totonicapán\",\"Totonicapán\":\"Totonicapán\",\"Zacapa\":\"Zacapa\"\n",
    "}\n",
    "\n",
    "map_cod  = F.create_map([F.lit(x) for x in chain(*{str(k):v for k,v in CODIGOS_DEPARTAMENTOS.items()}.items())])\n",
    "map_name = F.create_map([F.lit(x) for x in chain(*EQUIVALENCIAS_DEPARTAMENTOS.items())])\n",
    "\n",
    "def decodificar_depto(df, col=\"depto\"):\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "    s = F.col(col).cast(\"string\")\n",
    "    # quitar .0 si viene '1.0', '22.0', ...\n",
    "    s_no_dot = F.regexp_replace(s, \"\\\\.0+$\", \"\")\n",
    "    # si tras quitar .0 quedó solo dígitos => usar código; si no, mapear nombre\n",
    "    only_extras = F.translate(s_no_dot, \"0123456789\", \"\")\n",
    "    nombre = F.when(F.length(only_extras) == 0, map_cod.getItem(s_no_dot)) \\\n",
    "              .otherwise(map_name.getItem(s))\n",
    "    return df.withColumn(col, F.coalesce(nombre, F.lit(\"Desconocido\")))\n",
    "\n",
    "\n",
    "hechos_std     = decodificar_depto(hechos_std, \"depto\")\n",
    "vehiculos_std  = decodificar_depto(vehiculos_std, \"depto\")\n",
    "fallecidos_std = decodificar_depto(fallecidos_std, \"depto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58f84f7-e97f-474b-a8d0-5c95a2b4a8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "INT_COLS = [\n",
    "    \"anio\",\"mes\",\"dia\",\"dia_sem\",\"hora\",\"g_hora\",\"depto\",\"mupio\",\"zona\",\"area\",\n",
    "    \"tipo_accidente\",\"causa_acc\",\"sexo_pil\",\"edad_pil\",\"g_edad\",\"mayor_menor\",\n",
    "    \"tipo_veh\",\"marca_veh\",\"color_veh\",\"modelo_veh\",\"g_modelo_veh\",\"estado_pil\"\n",
    "]\n",
    "\n",
    "TOKENS_NULOS_LIST = [\n",
    "    \"ignorada\",\"na\",\"n/a\",\"s/d\",\"sd\",\"sin dato\",\"\",\" \",\n",
    "    \"Ignorada\",\"IGNORADA\",\"NA\",\"N/A\",\"S/D\",\"SD\",\"Sin dato\",\"SIN DATO\"\n",
    "]\n",
    "\n",
    "# Si quieres COERCIONAR rangos \"2010-2019\" a 2010, deja esto en True; si no, False para poner NULL\n",
    "COERCE_RANGES_TO_START = True\n",
    "\n",
    "def to_int_strict(colname: str):\n",
    "    s = F.trim(F.col(colname).cast(\"string\"))\n",
    "    s = F.when(F.lower(s).isin([t.lower() for t in TOKENS_NULOS_LIST]), None).otherwise(s)\n",
    "    # Quitar terminaciones .0, .00...\n",
    "    s = F.regexp_replace(s, r\"\\.0+$\", \"\")\n",
    "    # Opcional: si viene \"2010-2019\", toma el primer año; si no quieres, comenta esta línea\n",
    "    if COERCE_RANGES_TO_START:\n",
    "        s = F.regexp_replace(s, r\"^(\\d{4})-\\d{4}$\", r\"\\1\")\n",
    "\n",
    "    # Solo enteros válidos: - opcional AL INICIO y dígitos\n",
    "    s = F.when(s.rlike(r\"^-?\\d+$\"), s).otherwise(None)\n",
    "    return s.cast(T.IntegerType())\n",
    "\n",
    "def normalize_int_columns(df, int_cols):\n",
    "    for c in int_cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, to_int_strict(c))\n",
    "    # Regla específica: 9999 en modelo_veh → NULL\n",
    "    if \"modelo_veh\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"modelo_veh\",\n",
    "            F.when(F.col(\"modelo_veh\") == 9999, None).otherwise(F.col(\"modelo_veh\"))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "hechos_std     = normalize_int_columns(hechos_std, INT_COLS)\n",
    "vehiculos_std  = normalize_int_columns(vehiculos_std, INT_COLS)\n",
    "fallecidos_std = normalize_int_columns(fallecidos_std, INT_COLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e539c515-4a55-40bd-b665-376dce50d521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OUT_SILVER_HECHOS     = \"/Volumes/workspace/default/hechos_transito/silver\"\n",
    "OUT_SILVER_VEHICULOS  = \"/Volumes/workspace/default/vehiculos_involucrados/silver\"\n",
    "OUT_SILVER_FALLECIDOS = \"/Volumes/workspace/default/fallecidos_lesionados/silver\"\n",
    "\n",
    "dfs = [hechos_std, vehiculos_std, fallecidos_std]  # ojo: orden consistente\n",
    "OUTS = [OUT_SILVER_HECHOS, OUT_SILVER_VEHICULOS, OUT_SILVER_FALLECIDOS]\n",
    "\n",
    "for df, outp in zip(dfs, OUTS):\n",
    "    (df.coalesce(1)\n",
    "       .write.mode(\"overwrite\")\n",
    "       .parquet(outp))\n",
    "    print(\"OK →\", outp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "116815e8-ea38-4277-bae4-02473c7d9494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Helpers reutilizables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c84e375-2c6b-4029-a161-56ff5bdb174a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preguntas a responder\n",
    "1. Contar registros por tabla (long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a41ebe-e411-4a1f-a68c-61496b1edbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## #1 – Conteos, .show(), describe y summary (por tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94f7ec7-3025-43d1-bd43-01b75c2eb39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "hechos_silver = spark.read.parquet(OUT_SILVER_HECHOS)\n",
    "vehiculos_silver = spark.read.parquet(OUT_SILVER_VEHICULOS)\n",
    "fallecidos_silver = spark.read.parquet(OUT_SILVER_FALLECIDOS)\n",
    "\n",
    "def ae_conteos_describe_summary(nombre: str, sdfs: dict, n_show: int = 5):\n",
    "    print(f\"\\n===== {nombre.upper()} =====\")\n",
    "    total_registros = 0\n",
    "    for key, sdf in sdfs.items():\n",
    "        c = sdf.count()\n",
    "        total_registros += c\n",
    "        print(f\"{key:20s} -> {c:6d} registros\")\n",
    "    print(f\"TOTAL {nombre}: {total_registros}\")\n",
    "\n",
    "    # Muestra de una hoja representativa (la primera)\n",
    "    if sdfs:\n",
    "        first_key = list(sdfs.keys())[0]\n",
    "        print(f\"\\n--- Ejemplo .show() :: {first_key} ---\")\n",
    "        sdfs[first_key].show(n_show, truncate=False, vertical=True)\n",
    "\n",
    "        # describe/summary de columnas numéricas\n",
    "        num_cols = [f.name for f in sdfs[first_key].schema.fields if isinstance(f.dataType, NumericType)]\n",
    "        if num_cols:\n",
    "            print(f\"\\n--- describe(numéricas) :: {first_key} ---\")\n",
    "            sdfs[first_key].select(*num_cols).describe().show(truncate=False, vertical=True)\n",
    "\n",
    "            print(f\"\\n--- summary(numéricas) :: {first_key} ---\")\n",
    "            sdfs[first_key].select(*num_cols).summary(\"count\",\"mean\",\"stddev\",\"min\",\"25%\",\"50%\",\"75%\",\"max\").show(truncate=False, vertical=True)\n",
    "        else:\n",
    "            print(f\"\\n--- {first_key}: no hay columnas numéricas detectadas ---\")\n",
    "\n",
    "ae_conteos_describe_summary(\"hechos\", {\"hechos\": hechos_silver})\n",
    "ae_conteos_describe_summary(\"vehiculos\", {\"vehiculos\": vehiculos_silver})\n",
    "ae_conteos_describe_summary(\"fallecidos\", {\"fallecidos\": fallecidos_silver})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9f4429-aa75-46e1-916e-d5ce6ccb86da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## #2 – Años disponibles por tabla y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d30887e-f177-47ba-a250-9bdc965b0e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "EXPECTED_YEARS = set(range(2013, 2024))  # 2013..2023\n",
    "YEAR_RX = re.compile(r'(?<!\\d)(20\\d{2})(?:\\.0)?(?!\\d)')\n",
    "\n",
    "def extract_year_from_col(colname: str) -> int | None:\n",
    "    m = YEAR_RX.search(str(colname))\n",
    "    if not m: return None\n",
    "    try:\n",
    "        return int(m.group(1))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def detect_years_in_sdf(df) -> set[int]:\n",
    "    \"\"\"Detecta años en nombres de columnas de un Spark DataFrame.\"\"\"\n",
    "    years = set()\n",
    "    for c in df.columns:\n",
    "        y = extract_year_from_col(c)\n",
    "        if y:\n",
    "            years.add(y)\n",
    "    return years\n",
    "\n",
    "def report_years_df(df, titulo: str):\n",
    "    print(f\"\\n===== Verificación de años: {titulo} =====\")\n",
    "    found = detect_years_in_sdf(df)\n",
    "    missing = sorted(EXPECTED_YEARS - found)\n",
    "    outside = sorted(y for y in found if y not in EXPECTED_YEARS)\n",
    "    print(f\"Encontrados: {sorted(found) if found else '—'} | \"\n",
    "          f\"faltantes vs 2013–2023: {missing if missing else 'ninguno'} | \"\n",
    "          f\"fuera de rango: {outside if outside else 'ninguno'}\")\n",
    "    return found\n",
    "\n",
    "years_hechos     = report_years_df(hechos_silver, \"hechos\")\n",
    "years_vehiculos  = report_years_df(vehiculos_silver, \"vehiculos\")\n",
    "years_fallecidos = report_years_df(fallecidos_silver, \"fallecidos\")\n",
    "\n",
    "print(\"\\n¿Coinciden los conjuntos de años (intersección)?\")\n",
    "intersection_all = years_hechos & years_vehiculos & years_fallecidos\n",
    "print(\"Intersección común:\", sorted(intersection_all) if intersection_all else \"—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c520ac04-b689-4c5e-a959-7aafa3d4abd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### #3 – Valores distintos de 'tipo de accidente'   (buscamos columnas candidatas por nombre aproximado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7248bd6-9365-4bcf-8328-b1848c56981d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\n",
    "\n",
    "# --- normalizadores ---\n",
    "def _norm_text(s):\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")  # quita acentos\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _norm_colname(s):\n",
    "    if s is None: return \"\"\n",
    "    t = str(s).strip().lower()\n",
    "    t = (t.replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\")\n",
    "           .replace(\"ó\",\"o\").replace(\"ú\",\"u\").replace(\"ñ\",\"n\"))\n",
    "    t = re.sub(r\"[\\s\\-]+\", \"_\", t)\n",
    "    return t\n",
    "\n",
    "# --- catálogo canónico ---\n",
    "CANON = [\"colision\",\"Atropello\",\"Derrape\",\"Choque\",\"Vuelco\",\"Embarrancó\",\"Encunetó\",\"Caída\",\"Ignorado\"]\n",
    "MAP_NORM_TO_CANON = {\n",
    "    \"colision\":\"colision\",\"colisiones\":\"colision\",\"colision multiple\":\"colision\",\n",
    "    \"atropello\":\"Atropello\",\"atropellos\":\"Atropello\",\n",
    "    \"derrape\":\"Derrape\",\"derrapes\":\"Derrape\",\n",
    "    \"choque\":\"Choque\",\"choques\":\"Choque\",\n",
    "    \"vuelco\":\"Vuelco\",\"vuelcos\":\"Vuelco\",\n",
    "    \"embarranco\":\"Embarrancó\",\"embarranco multiple\":\"Embarrancó\",\n",
    "    \"encuneto\":\"Encunetó\",\"encunetamiento\":\"Encunetó\",\n",
    "    \"caida\":\"Caída\",\"caidas\":\"Caída\",\n",
    "    \"ignorado\":\"Ignorado\",\"desconocido\":\"Ignorado\",\"no especificado\":\"Ignorado\",\n",
    "    \"sin dato\":\"Ignorado\",\"na\":\"Ignorado\",\"n/a\":\"Ignorado\",\n",
    "}\n",
    "\n",
    "# --- UDF Spark para canonizar ---\n",
    "@F.udf(returnType=StringType())\n",
    "def canon_val_udf(val):\n",
    "    if val is None: return None\n",
    "    n = _norm_text(val)\n",
    "    if not n: return None\n",
    "    return MAP_NORM_TO_CANON.get(n, \"Ignorado\")\n",
    "\n",
    "# --- esquema estándar ---\n",
    "SCHEMA_ACCIDENTES = StructType([\n",
    "    StructField(\"sheet\", StringType(), True),\n",
    "    StructField(\"accidente\", StringType(), True),\n",
    "    StructField(\"count\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# --- funciones auxiliares ---\n",
    "def _find_acc_col(sdf):\n",
    "    \"\"\"Busca columna tipo_de_accidente (long)\"\"\"\n",
    "    for c in sdf.columns:\n",
    "        nc = _norm_colname(c)\n",
    "        if nc == \"tipo_de_accidente\" or nc.startswith(\"tipo_de_accid\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def accidentes_tidy_spark(sdf, sheet_name: str):\n",
    "    \"\"\"\n",
    "    Construye DataFrame tidy (sheet, accidente, count) desde Spark DF,\n",
    "    detectando automáticamente formato long + wide.\n",
    "    \"\"\"\n",
    "    # --- formato long ---\n",
    "    tipo_col = _find_acc_col(sdf)\n",
    "    if tipo_col:\n",
    "        df = sdf.withColumn(\"__tipo__\", canon_val_udf(F.col(tipo_col)))\n",
    "        # columnas numéricas candidatas\n",
    "        num_cols = [c for c,t in sdf.dtypes if t in (\"int\",\"bigint\",\"double\",\"float\") and c != tipo_col]\n",
    "        if not num_cols:\n",
    "            # si no hay columnas numéricas, devuelve vacío\n",
    "            return spark.createDataFrame([], schema=SCHEMA_ACCIDENTES)\n",
    "        # sumar por tipo\n",
    "        exprs = [F.coalesce(F.col(c).cast(DoubleType()), F.lit(0.0)) for c in num_cols]\n",
    "        total_expr = sum(exprs)\n",
    "        tidy = (df.groupBy(\"__tipo__\")\n",
    "                  .agg(F.sum(total_expr).alias(\"count\"))\n",
    "                  .withColumnRenamed(\"__tipo__\", \"accidente\")\n",
    "                  .withColumn(\"sheet\", F.lit(sheet_name))\n",
    "                  .select(\"sheet\",\"accidente\",\"count\"))\n",
    "        return tidy\n",
    "\n",
    "    # --- formato wide ---\n",
    "    # columnas que correspondan a CANON\n",
    "    cols_to_sum = [c for c in sdf.columns if _norm_text(c) in MAP_NORM_TO_CANON]\n",
    "    if not cols_to_sum:\n",
    "        return spark.createDataFrame([], schema=SCHEMA_ACCIDENTES)\n",
    "\n",
    "    exprs = [F.coalesce(F.col(c).cast(DoubleType()), F.lit(0.0)).alias(MAP_NORM_TO_CANON[_norm_text(c)]) for c in cols_to_sum]\n",
    "    df_sum = sdf.select(*exprs).groupBy().sum().collect()[0].asDict()\n",
    "    tidy_list = [(sheet_name, k, float(df_sum.get(k,0.0))) for k in CANON]\n",
    "    return spark.createDataFrame(tidy_list, schema=SCHEMA_ACCIDENTES)\n",
    "\n",
    "# --- USO ---\n",
    "acc_hechos_sdf     = accidentes_tidy_spark(hechos_silver, \"hechos\")\n",
    "acc_vehiculos_sdf  = accidentes_tidy_spark(vehiculos_silver, \"vehiculos\")\n",
    "acc_fallecidos_sdf = accidentes_tidy_spark(fallecidos_silver, \"fallecidos\")\n",
    "\n",
    "# --- Ejemplos de consulta ---\n",
    "print(\"\\nTop 10 (hechos) por count:\")\n",
    "(acc_hechos_sdf\n",
    " .groupBy(\"accidente\")\n",
    " .agg(F.sum(\"count\").alias(\"total\"))\n",
    " .orderBy(F.desc(\"total\"))\n",
    " .show(10, truncate=False))\n",
    "\n",
    "print(\"\\nHojas que sólo tuvieron 'Ignorado' (vehículos):\")\n",
    "from functools import reduce\n",
    "conds = [F.col(c).isNull() | (F.col(c)==0) for c in CANON if c!=\"Ignorado\"]\n",
    "(acc_vehiculos_sdf\n",
    " .groupBy(\"sheet\")\n",
    " .pivot(\"accidente\")\n",
    " .agg(F.sum(\"count\"))\n",
    " .where(F.col(\"Ignorado\").isNotNull() & reduce(lambda a,b: a & b, conds))\n",
    " .select(\"sheet\",\"Ignorado\")\n",
    " .show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa2a743-5189-4fd7-b0b4-162b44d8c37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### #4 – # de departamentos únicos por base(detecta columna 'departamento' aproximada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55136fb-cc2f-4978-b86e-4e65d5f956dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_departamentos_unique(dict_sdfs: dict, titulo: str,\n",
    "                               dep_cands=(\"departamento\",\"departamentos\",\"depto\",\"dept\",\"depart\",\"depar\")):\n",
    "    print(f\"\\n===== Departamentos únicos :: {titulo} =====\")\n",
    "    deptos = set()\n",
    "    cols_encontradas = 0\n",
    "\n",
    "    # Aux: si no tienes ya definida find_first_column, deja esto aquí\n",
    "    def find_first_column(candidates: list[str], columns: list[str]) -> str | None:\n",
    "        for col in columns:\n",
    "            low = col.lower()\n",
    "            for cand in candidates:\n",
    "                if cand in low:  # match por substring\n",
    "                    return col\n",
    "        return None\n",
    "\n",
    "    for key, sdf in dict_sdfs.items():\n",
    "        col = find_first_column(list(dep_cands), sdf.columns)\n",
    "        if not col:\n",
    "            continue\n",
    "        cols_encontradas += 1\n",
    "\n",
    "        vals = (sdf\n",
    "                .select(F.col(col).cast(\"string\").alias(\"departamento\"))\n",
    "                .where(F.col(\"departamento\").isNotNull() & (F.col(\"departamento\") != \"\"))\n",
    "                .distinct()\n",
    "                .toPandas()[\"departamento\"])\n",
    "\n",
    "        for v in vals:\n",
    "            if v is not None:\n",
    "                deptos.add(str(v).strip())\n",
    "\n",
    "    print(f\"Total únicos (unión de hojas): {len(deptos)}\")\n",
    "    print(f\"Columnas 'departamento' detectadas en {cols_encontradas} hojas\")\n",
    "    if deptos:\n",
    "        print(\"Ejemplos:\", sorted(list(deptos))[:15])\n",
    "count_departamentos_unique(hechos_sdfs, \"hechos\")\n",
    "count_departamentos_unique(vehiculos_sdfs, \"vehiculos\")\n",
    "count_departamentos_unique(lesionados_sdfs, \"lesionados\")\n",
    "count_departamentos_unique(fallecidos_sdfs, \"fallecidos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2171a7-06ad-48f2-9f82-d8cb9bfed222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuál es el total de accidentes por año y departamento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeef8a38-0ce7-4c50-9b18-b80d3ce4c283",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"total_accidentes\":{\"format\":{\"preset\":\"number-preset-standard\",\"config\":{\"type\":\"number-plain\",\"abbreviation\":\"none\",\"decimalPlaces\":{\"type\":\"max\",\"places\":2},\"hideGroupSeparator\":false}}}}},\"syncTimestamp\":1759556178113}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"YWNjX3Bvcl9hbmlvX2RlcCA9IChoZWNob3Nfc2lsdmVyCiAgICAgICAgICAgICAgICAgICAgLmdyb3VwQnkoImFuaW8iLCAiZGVwdG8iKQogICAgICAgICAgICAgICAgICAgIC5hZ2coRi5jb3VudCgiKiIpLmFsaWFzKCJ0b3RhbF9hY2NpZGVudGVzIikpCiAgICAgICAgICAgICAgICAgICAgKQoKZGlzcGxheShhY2NfcG9yX2FuaW9fZGVwKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView26b45d4\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView26b45d4\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView26b45d4\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView26b45d4) SELECT `depto`,SUM(`total_accidentes`) `column_ffd8cc2185`,`anio` FROM q GROUP BY `anio`,`depto`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView26b45d4\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "anio",
             "id": "column_b381ce0689"
            },
            "x": {
             "column": "depto",
             "id": "column_b381ce0685"
            },
            "y": [
             {
              "column": "total_accidentes",
              "id": "column_ffd8cc2185",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_b381ce0687": {
             "type": "column",
             "yAxis": 0
            },
            "column_ffd8cc2185": {
             "type": "column",
             "yAxis": 0,
             "zIndex": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "Total de accidentes"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "title": {
              "text": null
             },
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "59f45bf6-84f8-4074-9d33-0e3352389a51",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 23.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "depto",
           "type": "column"
          },
          {
           "column": "anio",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "depto",
           "type": "column"
          },
          {
           "alias": "column_ffd8cc2185",
           "args": [
            {
             "column": "total_accidentes",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "anio",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_por_anio_dep = (hechos_silver\n",
    "                    .groupBy(\"anio\", \"depto\")\n",
    "                    .agg(F.count(\"*\").alias(\"total_accidentes\"))\n",
    "                    )\n",
    "\n",
    "display(acc_por_anio_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ab6450-a7ee-4094-8863-be30e1b1b996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. ¿Qué día de la semana registra más accidentes en 2024?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622dc78f-b890-4e25-ac6b-202d7cd471cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBTZSBjYW1iaWEgMjAyNCBwb3IgMjAyMyBkZWJpZG8gYSBxdWUgbm8gc2UgdGllbmVuIGxvcyBkYXRvcyBkZWwgMjAyNApoZWNob3Nfc3RkLnNlbGVjdCgiZGlhX3NlbSIsICJhbmlvIikud2hlcmUoRi5jb2woImFuaW8iKSA9PSAiMjAyMyIpLmdyb3VwQnkoImRpYV9zZW0iKS5jb3VudCgpLmRpc3BsYXkoKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView92e9308\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView92e9308\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView92e9308\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView92e9308) SELECT `dia_sem`,SUM(`count`) `column_b381ce06103` FROM q GROUP BY `dia_sem`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView92e9308\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "dia_sem",
             "id": "column_b381ce06102"
            },
            "y": [
             {
              "column": "count",
              "id": "column_b381ce06103",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_b381ce06103": {
             "name": "count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "e3f206e0-4755-411c-9026-13e5211451da",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 25.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "dia_sem",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "dia_sem",
           "type": "column"
          },
          {
           "alias": "column_b381ce06103",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se cambia 2024 por 2023 debido a que no se tienen los datos del 2024\n",
    "hechos_std.select(\"dia_sem\", \"anio\").where(F.col(\"anio\") == \"2023\").groupBy(\"dia_sem\").count().display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB 8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
