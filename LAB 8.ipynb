{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3698d1-880a-4c4d-be9a-e89d96d07651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Importar librerías y definir funciones base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cc9bc0-f6e4-4d60-ba7f-f8d6f9af28d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7c33f0-52c7-45a2-b49e-bc05babd962f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61efac13-cedb-4a8b-b954-2f3915a46890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, NumericType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Ruta de tu archivo\n",
    "excel_path = \"/Volumes/workspace/default/pncc/20250527162011hk9xzLjtlLyIqA5fF0FY3udjjRUQlTkq.xlsx\"\n",
    "\n",
    "# Lista de palabras para detectar encabezados\n",
    "header_list = [\n",
    "    \"departamento\",\"vehículo\",\"vehiculo\",\"mes de ocurrencia\",\"día de la semana\",\"día de ocurrencia\",\n",
    "    \"hora de ocurrencia\",\"condición del conductor y sexo\",\"grupos de edad\",\n",
    "    \"tipo de vehículo y sexo de la persona\",\"zona de ocurrencia\",\"tipo de accidente y sexo\",\n",
    "    \"tipo\",\"clase\",\"marca\",\"modelo de vehículo\",\"sexo\",\"grupo\",\"edad\",\"tipo de vehículo\",\n",
    "    \"clases de vehículos\",\"color de vehículo\"\n",
    "]\n",
    "\n",
    "# Función para detectar y limpiar cuadros\n",
    "def read_cuadro(sheet_num, categoria):\n",
    "    sheet = f\"cuadro {sheet_num}\"\n",
    "    raw = pd.read_excel(excel_path, sheet_name=sheet, header=None)\n",
    "\n",
    "    # Buscar fila de encabezado\n",
    "    header_idx = None\n",
    "    for i in range(len(raw)):\n",
    "        row = raw.iloc[i].astype(str).str.lower()\n",
    "        if any(word in row.tolist() for word in header_list):\n",
    "            header_idx = i\n",
    "            break\n",
    "\n",
    "    if header_idx is None:\n",
    "        return None\n",
    "\n",
    "    header_row = raw.iloc[header_idx].tolist()\n",
    "    next_row   = raw.iloc[header_idx + 1].tolist()\n",
    "\n",
    "    # Combinar encabezados\n",
    "    combined_header = []\n",
    "    for h, n in zip(header_row, next_row):\n",
    "        if pd.isna(h) or str(h).strip() == \"\":\n",
    "            combined_header.append(n)\n",
    "        elif str(h).lower() in [\"año de ocurrencia\", \"mes de ocurrencia\",\"tipo de accidente\", \n",
    "                                \"día de la semana\",\"total\",\"hombre\",\"mujer\",\"ignorado\",\"grupos de edad\"]:\n",
    "            combined_header.append(n)\n",
    "        else:\n",
    "            combined_header.append(h)\n",
    "\n",
    "    df = raw.iloc[header_idx + 2:].copy()\n",
    "    df.columns = combined_header\n",
    "\n",
    "    # Limpieza\n",
    "    df = df.dropna(how=\"all\")\n",
    "    df = df[~df.iloc[:,0].astype(str).str.contains(\"fuente|nota|cuadro|serie\", case=False, na=False)]\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    # Agregar metadata\n",
    "    df[\"categoria\"] = categoria\n",
    "    df[\"cuadro\"] = sheet_num\n",
    "\n",
    "    return df\n",
    "\n",
    "# Función para procesar varios cuadros\n",
    "def build_dict(rango, categoria):\n",
    "    cuadros = {}\n",
    "    for i in rango:\n",
    "        try:\n",
    "            df = read_cuadro(i, categoria)\n",
    "            if df is not None:\n",
    "                cuadros[f\"{categoria}_{i}\"] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error en cuadro {i}: {e}\")\n",
    "    return cuadros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943e93f2-79e5-4c1d-a016-d46d687b0d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hechos_dfs     = build_dict(range(1, 17), \"hechos\")\n",
    "vehiculos_dfs  = build_dict(range(17, 29), \"vehiculos\")\n",
    "lesionados_dfs = build_dict(range(31, 47), \"lesionados\")\n",
    "fallecidos_dfs = build_dict(range(47, 63), \"fallecidos\")\n",
    "\n",
    "print(\"Hechos:\", list(hechos_dfs.keys())[:3])\n",
    "print(\"Vehículos:\", list(vehiculos_dfs.keys())[:3])\n",
    "print(\"Lesionados:\", list(lesionados_dfs.keys())[:3])\n",
    "print(\"Fallecidos:\", list(fallecidos_dfs.keys())[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d13c0a1-dd11-442c-84a2-873baae1f646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Funciones auxiliares para normalizar columnas y pasar a Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47db670a-63c3-4b27-909d-baa135c0db7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def safe_col(name: str) -> str:\n",
    "    n = str(name).strip()\n",
    "    n = re.sub(r'\\.0$', '', n)\n",
    "    n = n.replace('%','pct')\n",
    "    n = re.sub(r'\\s+','_', n)\n",
    "    n = re.sub(r'[^0-9a-zA-Z_]', '_', n)\n",
    "    if re.match(r'^\\d', n):\n",
    "        n = f\"y{n}\"\n",
    "    return n\n",
    "\n",
    "def make_unique(cols):\n",
    "    seen, out = {}, []\n",
    "    for c in cols:\n",
    "        base = c if c else \"col\"\n",
    "        if base not in seen:\n",
    "            seen[base] = 0\n",
    "            out.append(base)\n",
    "        else:\n",
    "            seen[base]+=1\n",
    "            out.append(f\"{base}_{seen[base]}\")\n",
    "    return out\n",
    "\n",
    "def to_scalar(x):\n",
    "    if isinstance(x,(list,dict,tuple,set)):\n",
    "        return str(x)\n",
    "    return x\n",
    "\n",
    "# Función pipeline: limpia y convierte a Spark DF\n",
    "def df_to_spark(df: pd.DataFrame):\n",
    "    # quitar filas con 'total'\n",
    "    mask_total = df.apply(lambda r: r.astype(str).str.strip().str.lower().eq(\"total\").any(), axis=1)\n",
    "    df_clean = df.loc[~mask_total].copy()\n",
    "\n",
    "    df_clean.columns = [safe_col(c) for c in df_clean.columns]\n",
    "    df_clean = df_clean.dropna(axis=1, how='all')\n",
    "    df_clean.columns = make_unique(df_clean.columns)\n",
    "    df_clean = df_clean.applymap(to_scalar)\n",
    "    df_clean = df_clean.where(pd.notnull(df_clean), None)\n",
    "\n",
    "    # detectar columnas numéricas\n",
    "    col_is_numeric = {}\n",
    "    for c in df_clean.columns:\n",
    "        ser = pd.to_numeric(df_clean[c], errors=\"coerce\")\n",
    "        ratio = ser.notna().mean() if len(ser) else 0.0\n",
    "        if ratio >= 0.8:\n",
    "            df_clean[c] = ser.astype(float)\n",
    "            col_is_numeric[c] = True\n",
    "        else:\n",
    "            df_clean[c] = df_clean[c].astype(\"string\")\n",
    "            col_is_numeric[c] = False\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(c, DoubleType() if col_is_numeric.get(c,False) else StringType(), True)\n",
    "        for c in df_clean.columns\n",
    "    ])\n",
    "\n",
    "    return spark.createDataFrame(df_clean.astype(object), schema=schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed9eb318-da6a-47db-b057-f28b48cc6a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Verificar años en cada colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc033a5-b194-4b15-abb9-01712dcbd5c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EXPECTED_YEARS = set(range(2020,2025))\n",
    "YEAR_RX = re.compile(r'(?<!\\d)(20\\d{2})(?:\\.0)?(?!\\d)')\n",
    "\n",
    "def extract_year_from_col(col):\n",
    "    m = YEAR_RX.search(str(col))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def detect_years_in_df(df: pd.DataFrame):\n",
    "    return {y for c in df.columns if (y:=extract_year_from_col(c))}\n",
    "\n",
    "def report_years_for_dict(dfs, title):\n",
    "    print(f\"\\n===== {title} =====\")\n",
    "    for key,df in dfs.items():\n",
    "        found = detect_years_in_df(df)\n",
    "        missing = sorted(EXPECTED_YEARS - found)\n",
    "        outside = sorted(y for y in found if y not in EXPECTED_YEARS)\n",
    "        print(f\"{key}: {sorted(found) if found else '—'} | faltantes={missing or 'ninguno'} | fuera={outside or 'ninguno'}\")\n",
    "\n",
    "# Ejecutar reportes\n",
    "report_years_for_dict(hechos_dfs, \"hechos\")\n",
    "report_years_for_dict(vehiculos_dfs, \"vehiculos\")\n",
    "report_years_for_dict(lesionados_dfs, \"lesionados\")\n",
    "report_years_for_dict(fallecidos_dfs, \"fallecidos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54a41ebe-e411-4a1f-a68c-61496b1edbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Ejemplo: mostrar Spark DF con estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94f7ec7-3025-43d1-bd43-01b75c2eb39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejemplo con hechos_1\n",
    "sdf_hechos = df_to_spark(hechos_dfs[\"hechos_1\"])\n",
    "print(\"Total registros:\", sdf_hechos.count())\n",
    "\n",
    "sdf_hechos.show(10, truncate=False)\n",
    "\n",
    "# describe y summary de columnas numéricas\n",
    "num_cols = [f.name for f in sdf_hechos.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "if num_cols:\n",
    "    sdf_hechos.select(*num_cols).describe().show()\n",
    "    sdf_hechos.select(*num_cols).summary(\"count\",\"mean\",\"stddev\",\"min\",\"25%\",\"50%\",\"75%\",\"max\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB 8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
