{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07da6481-db74-495c-984b-d43f7e2b697d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "EL REPO ES ENCONTRADO ACA: https://github.com/DiegoLinares11/Lab8-Databricks \n",
    "\n",
    "\n",
    "DIEGO LINARES\n",
    "\n",
    "\n",
    "JOSE PRINCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3698d1-880a-4c4d-be9a-e89d96d07651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Importar librerías y definir funciones base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cc9bc0-f6e4-4d60-ba7f-f8d6f9af28d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "%pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7c33f0-52c7-45a2-b49e-bc05babd962f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff737ad-c5a3-4d8e-a16e-c2e01074fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carga desde archivo y construcción dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943e93f2-79e5-4c1d-a016-d46d687b0d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, io\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "\n",
    "# Importante: pandas SI puede leer rutas locales si usas el prefijo /dbfs/...\n",
    "DIRS = [\n",
    "    \"/Volumes/workspace/default/fallecidos_lesionados/\",\n",
    "    \"/Volumes/workspace/default/hechos_transito/\",\n",
    "    \"/Volumes/workspace/default/vehiculos_involucrados/\",\n",
    "]\n",
    "\n",
    "for d in DIRS:\n",
    "    for fi in dbutils.fs.ls(d):\n",
    "        file_path = fi.path                      # ej: /Volumes/workspace/.../archivo.xlsx\n",
    "        file_path_dbfs = \"/dbfs\" + file_path     # ruta local (driver)\n",
    "        lower = fi.name.lower()\n",
    "        if lower.endswith(\".xlsx\"):\n",
    "            out_csv = file_path.replace(\".xlsx\", \".csv\")\n",
    "            try:\n",
    "                df = pd.read_excel(file_path_dbfs)   # lee Excel\n",
    "                df.to_csv(\"/dbfs\" + out_csv, index=False)\n",
    "                print(f\"XLSX→CSV: {fi.name}\")\n",
    "                dbutils.fs.rm(file_path)  # opcional: borra el Excel\n",
    "            except Exception as e:\n",
    "                print(\"Error XLSX\", fi.name, e)\n",
    "        elif lower.endswith(\".sav\"):\n",
    "            out_csv = file_path.replace(\".sav\", \".csv\")\n",
    "            try:\n",
    "                df, meta = pyreadstat.read_sav(file_path_dbfs, apply_value_formats=False)\n",
    "                df.to_csv(\"/dbfs\" + out_csv, index=False)\n",
    "                print(f\"SAV→CSV: {fi.name}\")\n",
    "                dbutils.fs.rm(file_path)  # opcional\n",
    "            except Exception as e:\n",
    "                print(\"Error SAV\", fi.name, e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15a6b2c-1cc0-4f2d-a320-fd46e3280130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unir TODOS los CSV de una carpeta en un solo DataFrame (robusto a columnas distintas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b0b017-f7cf-4256-802e-4a2d6e7f924a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "DIR_HECHOS     = \"/Volumes/workspace/default/hechos_transito\"\n",
    "DIR_VEHICULOS  = \"/Volumes/workspace/default/vehiculos_involucrados\"\n",
    "DIR_FALLECIDOS = \"/Volumes/workspace/default/fallecidos_lesionados\"\n",
    "\n",
    "def read_folder_csv_uc(path: str):\n",
    "    df = (spark.read\n",
    "          .option(\"header\", True)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .csv(f\"{path}/*.csv\"))\n",
    "    \n",
    "    # UC: usa _metadata.file_path en lugar de input_file_name()\n",
    "    if \"_metadata\" in df.columns:\n",
    "        df = df.withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    "    else:\n",
    "        # Fallback UC (si por algún motivo no aparece _metadata)\n",
    "        df = df.withColumn(\"source_file\", F.lit(None))\n",
    "    return df\n",
    "\n",
    "DIR_HECHOS     = \"/Volumes/workspace/default/hechos_transito\"\n",
    "DIR_VEHICULOS  = \"/Volumes/workspace/default/vehiculos_involucrados\"\n",
    "DIR_FALLECIDOS = \"/Volumes/workspace/default/fallecidos_lesionados\"\n",
    "\n",
    "hechos_raw     = read_folder_csv_uc(DIR_HECHOS)\n",
    "vehiculos_raw  = read_folder_csv_uc(DIR_VEHICULOS)\n",
    "fallecidos_raw = read_folder_csv_uc(DIR_FALLECIDOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5adc32-2e64-422a-9178-59d98711176b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Canoniza nombres (alias → nombre estándar)\n",
    "ALIASES = {\n",
    "    \"num_hecho\":      [\"num_hecho\",\"num\",\"num_correl\",\"núm_corre\",\"num_corre\",\"num_correlativo\"],\n",
    "    \"anio\":           [\"anio\",\"año\",\"ano\",\"anio_ocu\",\"año_ocu\",\"ano_ocu\"],\n",
    "    \"mes\":            [\"mes\",\"mes_ocu\"],\n",
    "    \"dia\":            [\"dia\",\"día\",\"dia_ocu\",\"día_ocu\"],\n",
    "    \"dia_sem\":        [\"dia_sem\",\"día_sem\",\"dia_sem_ocu\",\"día_sem_ocu\"],\n",
    "    \"hora\":           [\"hora\",\"hora_ocu\"],\n",
    "    \"g_hora\":         [\"g_hora\",\"g_hora_5\"],\n",
    "    \"depto\":          [\"depto\",\"depto_ocu\"],\n",
    "    \"mupio\":          [\"mupio\",\"mupio_ocu\",\"muni_ocu\",\"municipio\"],\n",
    "    \"zona\":           [\"zona\",\"zona_ocu\"],\n",
    "    \"area\":           [\"area\",\"areag_ocu\",\"area_ocu\"],\n",
    "    \"tipo_accidente\": [\"tipo_accidente\",\"tipo_eve\",\"tipo_evento\",\"tipo_acc\",\"tipo\"],\n",
    "    \"causa_acc\":      [\"causa_acc\",\"causa\"],\n",
    "    \"sexo_pil\":       [\"sexo_pil\",\"sexo_piloto\",\"sexo\",\"sexo_per\"],\n",
    "    \"edad_pil\":       [\"edad_pil\",\"edad\",\"edad_piloto\",\"edad_per\"],\n",
    "    \"g_edad\":         [\"g_edad\",\"g_edad_2\",\"g_edad_80ymás\",\"g_edad_60ymás\",\"edad_quinquenales\"],\n",
    "    \"mayor_menor\":    [\"mayor_menor\"],\n",
    "    \"tipo_veh\":       [\"tipo_veh\",\"tipo_vehiculo\"],\n",
    "    \"marca_veh\":      [\"marca_veh\",\"marca\"],\n",
    "    \"color_veh\":      [\"color_veh\",\"color\"],\n",
    "    \"modelo_veh\":     [\"modelo_veh\",\"modelo\"],\n",
    "    \"g_modelo_veh\":   [\"g_modelo_veh\"],\n",
    "    \"estado_pil\":     [\"estado_pil\",\"estado_piloto\",\"estado\",\"estado_con\",\"fall_les\"],\n",
    "    \"intencionalidad\":[\"int_o_noint\",\"intencionalidad\"],\n",
    "    \"source_file\":    [\"source_file\",\"_metadata.file_path\",\"_source_file\"]\n",
    "}\n",
    "\n",
    "def _first_present(cols, cands):\n",
    "    lc = {c.lower(): c for c in cols}\n",
    "    for x in cands:\n",
    "        if x.lower() in lc: return lc[x.lower()]\n",
    "    return None\n",
    "\n",
    "def canonize(df):\n",
    "    mapping = {}\n",
    "    for canon, cands in ALIASES.items():\n",
    "        src = _first_present(df.columns, cands)\n",
    "        if src and src != canon:\n",
    "            mapping[src] = canon\n",
    "    for src, dst in mapping.items():\n",
    "        df = df.withColumnRenamed(src, dst)\n",
    "    return df\n",
    "\n",
    "hechos_std     = canonize(hechos_raw)\n",
    "vehiculos_std  = canonize(vehiculos_raw)\n",
    "fallecidos_std = canonize(fallecidos_raw)\n",
    "\n",
    "# Si falta 'anio' en alguna hoja, extraerlo del nombre de archivo\n",
    "for name in [\"hechos_std\",\"vehiculos_std\",\"fallecidos_std\"]:\n",
    "    df = locals()[name]\n",
    "    if \"anio\" not in df.columns:\n",
    "        df = df.withColumn(\"anio\", F.regexp_extract(\"source_file\", r\"(20\\d{2})\", 1).cast(\"int\"))\n",
    "        locals()[name] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298fde75-e5cf-4b4d-bdd8-9e584a2c1f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2) Decodificación con diccionario de datos (genérico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4fb1ec-49d9-4d76-b1df-59fe7a3e085e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "DICC_DIR = \"/Volumes/workspace/default/diccionario\" \n",
    "\n",
    "def load_map(colname: str):\n",
    "    path = f\"{DICC_DIR}/{colname}.csv\"\n",
    "    try:\n",
    "        dim = (spark.read.option(\"header\", True).csv(path)\n",
    "               .select(F.col(\"codigo\").cast(\"string\").alias(\"k\"), F.col(\"etiqueta\").alias(\"v\")))\n",
    "        # create_map exige lista [k1,v1,k2,v2,...]\n",
    "        pairs = [F.lit(x) for r in dim.collect() for x in (r[\"k\"], r[\"v\"])]\n",
    "        return F.create_map(pairs) if pairs else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Fallback de departamentos (por si no cargas diccionario aún)\n",
    "CODIGOS_DEPARTAMENTOS = {\n",
    "  \"1\":\"Guatemala\",\"2\":\"El Progreso\",\"3\":\"Sacatepéquez\",\"4\":\"Chimaltenango\",\"5\":\"Escuintla\",\n",
    "  \"6\":\"Santa Rosa\",\"7\":\"Sololá\",\"8\":\"Totonicapán\",\"9\":\"Quetzaltenango\",\"10\":\"Suchitepéquez\",\n",
    "  \"11\":\"Retalhuleu\",\"12\":\"San Marcos\",\"13\":\"Huehuetenango\",\"14\":\"Quiché\",\"15\":\"Baja Verapaz\",\n",
    "  \"16\":\"Alta Verapaz\",\"17\":\"Petén\",\"18\":\"Izabal\",\"19\":\"Zacapa\",\"20\":\"Chiquimula\",\n",
    "  \"21\":\"Jalapa\",\"22\":\"Jutiapa\",\"99\":\"Desconocido\"\n",
    "}\n",
    "depto_map_fallback = F.create_map([F.lit(x) for kv in CODIGOS_DEPARTAMENTOS.items() for x in kv])\n",
    "\n",
    "def decode_column(df, colname):\n",
    "    # intenta usar diccionario; si no existe y la columna es 'depto', usa fallback\n",
    "    m = load_map(colname)\n",
    "    if m is None and colname == \"depto\":\n",
    "        m = depto_map_fallback\n",
    "    if m is None or colname not in df.columns:\n",
    "        return df\n",
    "    # normaliza a string y aplica mapeo\n",
    "    return (df\n",
    "      .withColumn(colname, \n",
    "          m.getItem(\n",
    "              F.when(F.col(colname).isNull(), F.lit(None))\n",
    "               .otherwise(F.regexp_replace(F.col(colname).cast(\"string\"), r\"\\.0$\", \"\")) # '1.0'->'1'\n",
    "          )\n",
    "      ))\n",
    "\n",
    "# Aplica decodificación a las columnas relevantes\n",
    "DECODE_COLS = [\"depto\",\"dia_sem\",\"g_hora\",\"tipo_accidente\",\"estado_pil\",\"intencionalidad\",\n",
    "               \"tipo_veh\",\"marca_veh\",\"color_veh\",\"g_edad\",\"g_modelo_veh\"]\n",
    "for name in [\"hechos_std\",\"vehiculos_std\",\"fallecidos_std\"]:\n",
    "    df = locals()[name]\n",
    "    for c in DECODE_COLS:\n",
    "        df = decode_column(df, c)\n",
    "    locals()[name] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "970adae6-006d-4015-91a6-67f222e8555f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Tipado robusto (sin romper por rangos ‘2010-2019’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58f84f7-e97f-474b-a8d0-5c95a2b4a8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.types import NullType\n",
    "INT_LIKE = [\n",
    "    \"anio\",\"mes\",\"dia\",\"hora\",\"zona\",\"mupio\",\"area\"\n",
    "    # ojo: \"modelo_veh\" puede ser año numérico; castear pero limpiando '9999'\n",
    "]\n",
    "\n",
    "def to_int_safe(colname: str):\n",
    "    # 1) a string y limpieza básica\n",
    "    s = F.col(colname).cast(\"string\")\n",
    "    s = F.regexp_replace(F.trim(s), r\"\\s+\", \"\")   # quita espacios\n",
    "    s = F.regexp_replace(s, r\"\\.0$\", \"\")          # '1.0' -> '1'\n",
    "    # 2) solo deja dígitos enteros (con signo opcional)\n",
    "    s = F.when(s.rlike(r\"^-?\\d+$\"), s).otherwise(None)\n",
    "    # 3) convierte a int\n",
    "    return s.cast(T.IntegerType())\n",
    "\n",
    "def clean_numeric(df):\n",
    "    for c in INT_LIKE:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, to_int_safe(c))\n",
    "    if \"modelo_veh\" in df.columns:\n",
    "        df = df.withColumn(\"modelo_veh\", to_int_safe(\"modelo_veh\"))\n",
    "        df = df.withColumn(\"modelo_veh\",\n",
    "                           F.when(F.col(\"modelo_veh\") == 9999, None)\n",
    "                            .otherwise(F.col(\"modelo_veh\")))\n",
    "    return df\n",
    "\n",
    "hechos_std     = clean_numeric(hechos_std)\n",
    "vehiculos_std  = clean_numeric(vehiculos_std)\n",
    "fallecidos_std = clean_numeric(fallecidos_std)\n",
    "\n",
    "def ensure_source_file_string(df):\n",
    "    if \"source_file\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"source_file\",\n",
    "            F.coalesce(F.col(\"source_file\").cast(T.StringType()), F.lit(\"\"))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "hechos_std     = ensure_source_file_string(hechos_std)\n",
    "vehiculos_std  = ensure_source_file_string(vehiculos_std)\n",
    "fallecidos_std = ensure_source_file_string(fallecidos_std)\n",
    "\n",
    "def drop_nulltype_cols(df):\n",
    "    null_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, NullType)]\n",
    "    return df.drop(*null_cols) if null_cols else df\n",
    "\n",
    "hechos_std     = drop_nulltype_cols(hechos_std)\n",
    "vehiculos_std  = drop_nulltype_cols(vehiculos_std)\n",
    "fallecidos_std = drop_nulltype_cols(fallecidos_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86c4da2-13cb-46c4-ab68-87c56204a2cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2013, 2024\n",
    "MES_MIN,  MES_MAX  = 1, 12\n",
    "DIA_MIN,  DIA_MAX  = 1, 31\n",
    "HORA_MIN, HORA_MAX = 0, 23\n",
    "ZONA_MIN,  ZONA_MAX = 1, 99\n",
    "\n",
    "def sanitize_calendar(df, try_fix_year_from_source=True):\n",
    "    if \"anio\" in df.columns:\n",
    "        # anio dentro de rango, si no → NULL\n",
    "        df = df.withColumn(\n",
    "            \"anio\",\n",
    "            F.when((F.col(\"anio\") >= YEAR_MIN) & (F.col(\"anio\") <= YEAR_MAX), F.col(\"anio\"))\n",
    "             .otherwise(F.lit(None).cast(\"int\"))\n",
    "        )\n",
    "        # recuperar desde source_file, evitando cast de \"\" → int\n",
    "        if try_fix_year_from_source and \"source_file\" in df.columns:\n",
    "            y_str = F.regexp_extract(F.col(\"source_file\"), r\"(20\\d{2})\", 1)\n",
    "            y_int = F.when(F.length(y_str) > 0, y_str.cast(\"int\")) \\\n",
    "                     .otherwise(F.lit(None).cast(\"int\"))\n",
    "            df = df.withColumn(\"anio\", F.coalesce(F.col(\"anio\"), y_int))\n",
    "\n",
    "    if \"mes\" in df.columns:\n",
    "        df = df.withColumn(\"mes\",  F.when((F.col(\"mes\")  >= MES_MIN)  & (F.col(\"mes\")  <= MES_MAX),  F.col(\"mes\")))\n",
    "    if \"dia\" in df.columns:\n",
    "        df = df.withColumn(\"dia\",  F.when((F.col(\"dia\")  >= DIA_MIN)  & (F.col(\"dia\")  <= DIA_MAX),  F.col(\"dia\")))\n",
    "    if \"hora\" in df.columns:\n",
    "        df = df.withColumn(\"hora\", F.when((F.col(\"hora\") >= HORA_MIN) & (F.col(\"hora\") <= HORA_MAX), F.col(\"hora\")))\n",
    "    if \"zona\" in df.columns:\n",
    "        df = df.withColumn(\"zona\", F.when((F.col(\"zona\") >= ZONA_MIN) & (F.col(\"zona\") <= ZONA_MAX), F.col(\"zona\")))\n",
    "\n",
    "    if \"modelo_veh\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"modelo_veh\",\n",
    "            F.when((F.col(\"modelo_veh\") >= 1900) & (F.col(\"modelo_veh\") <= 2030), F.col(\"modelo_veh\"))\n",
    "        )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e539c515-4a55-40bd-b665-376dce50d521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OUT_SILVER_HECHOS     = \"/Volumes/workspace/default/hechos_transito/silver\"\n",
    "OUT_SILVER_VEHICULOS  = \"/Volumes/workspace/default/vehiculos_involucrados/silver\"\n",
    "OUT_SILVER_FALLECIDOS = \"/Volumes/workspace/default/fallecidos_lesionados/silver\"\n",
    "\n",
    "for df, outp in [\n",
    "    (hechos_std, OUT_SILVER_HECHOS),\n",
    "    (vehiculos_std, OUT_SILVER_VEHICULOS),\n",
    "    (fallecidos_std, OUT_SILVER_FALLECIDOS),\n",
    "]:\n",
    "    (df.coalesce(1)\n",
    "       .write.mode(\"overwrite\")\n",
    "       .parquet(outp))\n",
    "    print(\"Silver →\", outp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e41363-3481-4860-b4da-69bb743784f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hechos_silver     = spark.read.parquet(OUT_SILVER_HECHOS)\n",
    "vehiculos_silver  = spark.read.parquet(OUT_SILVER_VEHICULOS)\n",
    "fallecidos_silver = spark.read.parquet(OUT_SILVER_FALLECIDOS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805b551b-6631-4f9e-abdd-6f2ca9c2d83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplica a tus SILVER cargados:\n",
    "hechos_silver     = sanitize_calendar(hechos_silver)\n",
    "vehiculos_silver  = sanitize_calendar(vehiculos_silver)\n",
    "fallecidos_silver = sanitize_calendar(fallecidos_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c849eef-e97d-4e90-8b97-d7b625fecf81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrega esto después de cargar tus silver para entender tu data\n",
    "def analizar_completitud(df, nombre):\n",
    "    print(f\"\\n===== {nombre} =====\")\n",
    "    total = df.count()\n",
    "    print(f\"Total registros: {total}\")\n",
    "    \n",
    "    cols_clave = [\"anio\", \"mes\", \"dia\", \"hora\", \"depto\", \"tipo_accidente\"]\n",
    "    for col in cols_clave:\n",
    "        if col in df.columns:\n",
    "            no_null = df.filter(F.col(col).isNotNull()).count()\n",
    "            pct = (no_null/total)*100\n",
    "            print(f\"{col:20s}: {no_null:6d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Combinaciones comunes\n",
    "    print(\"\\nCombinaciones completas:\")\n",
    "    print(f\"  anio+mes+depto:      {df.filter(F.col('anio').isNotNull() & F.col('mes').isNotNull() & F.col('depto').isNotNull()).count()}\")\n",
    "    print(f\"  anio+mes+dia+hora:   {df.filter(F.col('anio').isNotNull() & F.col('mes').isNotNull() & F.col('dia').isNotNull() & F.col('hora').isNotNull()).count()}\")\n",
    "\n",
    "analizar_completitud(hechos_silver, \"HECHOS\")\n",
    "analizar_completitud(vehiculos_silver, \"VEHICULOS\")\n",
    "analizar_completitud(fallecidos_silver, \"FALLECIDOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d637e44-10fb-49c4-96c6-0d6c474aaae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hechos_fecha_completa = hechos_silver.filter(\n",
    "    F.col(\"anio\").isNotNull() & \n",
    "    F.col(\"mes\").isNotNull() & \n",
    "    F.col(\"dia\").isNotNull()\n",
    ")\n",
    "\n",
    "# Vista 2: Registros con hora (más detallados)\n",
    "hechos_con_hora = hechos_silver.filter(\n",
    "    F.col(\"hora\").isNotNull()\n",
    ")\n",
    "\n",
    "# Vista 3: Resumen por año-mes-depto (para joins)\n",
    "hechos_mensual = hechos_silver.filter(\n",
    "    F.col(\"anio\").isNotNull() & \n",
    "    F.col(\"mes\").isNotNull() & \n",
    "    F.col(\"depto\").isNotNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "116815e8-ea38-4277-bae4-02473c7d9494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Helpers reutilizables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c84e375-2c6b-4029-a161-56ff5bdb174a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preguntas a responder\n",
    "1. Contar registros por tabla (long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a41ebe-e411-4a1f-a68c-61496b1edbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## #1 – Conteos, .show(), describe y summary (por tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94f7ec7-3025-43d1-bd43-01b75c2eb39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.sql import functions as F\n",
    "def ae_conteos_describe_summary(nombre: str, sdfs: dict, n_show: int = 5):\n",
    "    print(f\"\\n===== {nombre.upper()} =====\")\n",
    "    total_registros = 0\n",
    "    for key, sdf in sdfs.items():\n",
    "        c = sdf.count()\n",
    "        total_registros += c\n",
    "        print(f\"{key:20s} -> {c:6d} registros\")\n",
    "    print(f\"TOTAL {nombre}: {total_registros}\")\n",
    "\n",
    "    if sdfs:\n",
    "        first_key = list(sdfs.keys())[0]\n",
    "        print(f\"\\n--- Ejemplo .show() :: {first_key} ---\")\n",
    "        sdfs[first_key].show(n_show, truncate=False, vertical=True)\n",
    "\n",
    "        num_cols = [f.name for f in sdfs[first_key].schema.fields if isinstance(f.dataType, NumericType)]\n",
    "        if num_cols:\n",
    "            print(f\"\\n--- describe(numéricas) :: {first_key} ---\")\n",
    "            sdfs[first_key].select(*num_cols).describe().show(truncate=False, vertical=True)\n",
    "\n",
    "            print(f\"\\n--- summary(numéricas) :: {first_key} ---\")\n",
    "            sdfs[first_key].select(*num_cols).summary(\n",
    "                \"count\",\"mean\",\"stddev\",\"min\",\"25%\",\"50%\",\"75%\",\"max\"\n",
    "            ).show(truncate=False, vertical=True)\n",
    "        else:\n",
    "            print(f\"\\n--- {first_key}: no hay columnas numéricas detectadas ---\")\n",
    "\n",
    "ae_conteos_describe_summary(\"hechos\",     {\"hechos\": hechos_silver})\n",
    "ae_conteos_describe_summary(\"vehiculos\",  {\"vehiculos\": vehiculos_silver})\n",
    "ae_conteos_describe_summary(\"fallecidos\", {\"fallecidos\": fallecidos_silver})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9f4429-aa75-46e1-916e-d5ce6ccb86da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## #2 – Años disponibles por tabla y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d30887e-f177-47ba-a250-9bdc965b0e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EXPECTED_YEARS = set(range(2013, 2024))  # 2013..2023\n",
    "\n",
    "def report_years_col(df, titulo: str):\n",
    "    print(f\"\\n===== Verificación de años: {titulo} =====\")\n",
    "    found = sorted([r[0] for r in df.select(\"anio\").dropna().distinct().collect()])\n",
    "    found_set = set(found)\n",
    "    missing = sorted(EXPECTED_YEARS - found_set)\n",
    "    outside = sorted(y for y in found if y not in EXPECTED_YEARS)\n",
    "    print(f\"Encontrados: {found if found else '—'} | \"\n",
    "          f\"faltantes vs 2013–2023: {missing if missing else 'ninguno'} | \"\n",
    "          f\"fuera de rango: {outside if outside else 'ninguno'}\")\n",
    "    return found_set\n",
    "\n",
    "years_hechos     = report_years_col(hechos_silver, \"hechos\")\n",
    "years_vehiculos  = report_years_col(vehiculos_silver, \"vehiculos\")\n",
    "years_fallecidos = report_years_col(fallecidos_silver, \"fallecidos\")\n",
    "\n",
    "print(\"\\n¿Coinciden los conjuntos de años (intersección)?\")\n",
    "intersection_all = years_hechos & years_vehiculos & years_fallecidos\n",
    "print(\"Intersección común:\", sorted(intersection_all) if intersection_all else \"—\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c520ac04-b689-4c5e-a959-7aafa3d4abd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### #3 – Valores distintos de 'tipo de accidente'   (buscamos columnas candidatas por nombre aproximado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7248bd6-9365-4bcf-8328-b1848c56981d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ### #3 – Valores distintos de 'tipo de accidente'\n",
    "import unicodedata, re\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    return \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "def _norm_text(s):\n",
    "    if s is None: return None\n",
    "    s = str(s).strip()\n",
    "    s = _strip_accents(s).lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "    \n",
    "YEAR_RX = r\"^(19\\d{2}|20\\d{2})(\\.0)?$\"   # 1984 o 1984.0\n",
    "INT_FLOAT_RX = r\"^(\\d+)(?:\\.0)?$\"        # 1.0 -> 1\n",
    "\n",
    "def clean_tipo_accidente(df, col=\"tipo_accidente\"):\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "\n",
    "    s = F.trim(F.col(col).cast(\"string\"))\n",
    "    s = F.regexp_replace(s, INT_FLOAT_RX, r\"\\1\")\n",
    "    s = F.when(s.rlike(YEAR_RX), F.lit(None)).otherwise(s)\n",
    "\n",
    "    s = F.when(s == \"99\", F.lit(\"Ignorado\")).otherwise(s)\n",
    "\n",
    "    fixes = {\n",
    "        \"Colisi?n\":\"Colisión\", \"Embarranc?\":\"Embarrancó\",\n",
    "        \"Encunet?\":\"Encunetó\", \"Ca?da\":\"Caída\"\n",
    "    }\n",
    "    s = F.coalesce(*[F.when(s == bad, F.lit(good)) for bad, good in fixes.items()] + [s])\n",
    "\n",
    "    return df.withColumn(col, s)\n",
    "\n",
    "hechos_silver     = clean_tipo_accidente(hechos_silver, \"tipo_accidente\")\n",
    "vehiculos_silver  = clean_tipo_accidente(vehiculos_silver, \"tipo_accidente\")\n",
    "fallecidos_silver = clean_tipo_accidente(fallecidos_silver, \"tipo_accidente\")\n",
    "@F.udf(returnType=StringType())\n",
    "def tipo_canon(val):\n",
    "    n = _norm_text(val)\n",
    "    if not n: return None\n",
    "    syn = {\n",
    "            \"colisiones\":\"colision\", \"colision multiple\":\"colision\",\n",
    "            \"choques\":\"choque\", \"derrapes\":\"derrape\",\n",
    "            \"vuelcos\":\"vuelco\", \"embarranco multiple\":\"embarranco\",\n",
    "            \"encunetamiento\":\"encuneto\", \"encunetado\":\"encuneto\",\n",
    "            \"caidas\":\"caida\", \"no especificado\":\"ignorado\",\n",
    "            \"sin dato\":\"ignorado\", \"na\":\"ignorado\", \"n/a\":\"ignorado\"\n",
    "    }\n",
    "    return syn.get(n, n)\n",
    "\n",
    "def find_tipo_col(df):\n",
    "    cands = [c for c in df.columns if re.sub(r\"[\\s\\-\\_]+\",\"\", c.lower()) \\\n",
    "            in (\"tipodeaccidente\",\"tipoaccidente\",\"tipoacc\",\"tipo\",\"tipoeve\",\"tipoevento\")]\n",
    "    if cands: return cands[0]\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if \"tipo\" in lc and (\"accid\" in lc or \"eve\" in lc or \"evento\" in lc):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def distinct_tipos(df, nombre):\n",
    "    col = find_tipo_col(df)\n",
    "    if not col:\n",
    "        print(f\"[{nombre}] No encontré columna de tipo de accidente\"); \n",
    "        return None, None\n",
    "\n",
    "    print(f\"\\n[{nombre}] Columna detectada: {col}\")\n",
    "\n",
    "    print(f\"\\n[{nombre}] Distinct RAW (muestra hasta 100):\")\n",
    "    df.select(col).distinct().orderBy(col).show(100, truncate=False)\n",
    "\n",
    "    print(f\"\\n[{nombre}] Distinct NORMALIZADOS + conteo:\")\n",
    "    norm = (df\n",
    "            .withColumn(\"tipo_norm\", tipo_canon(F.col(col)))\n",
    "            .groupBy(\"tipo_norm\")\n",
    "            .count()\n",
    "            .orderBy(F.desc(\"count\"), F.asc(\"tipo_norm\")))\n",
    "    norm.show(100, truncate=False)\n",
    "    return col, norm\n",
    "\n",
    "col_hechos,     acc_hechos_norm     = distinct_tipos(hechos_silver, \"hechos\")\n",
    "col_vehiculos,  acc_vehiculos_norm  = distinct_tipos(vehiculos_silver, \"vehiculos\")\n",
    "col_fallecidos, acc_fallecidos_norm = distinct_tipos(fallecidos_silver, \"fallecidos\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa2a743-5189-4fd7-b0b4-162b44d8c37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### #4 – # de departamentos únicos por base(detecta columna 'departamento' aproximada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55136fb-cc2f-4978-b86e-4e65d5f956dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hechos\n",
    "deptos_hechos = hechos_silver.select(\"depto\").filter(F.col(\"depto\").isNotNull()).distinct()\n",
    "count_h = deptos_hechos.count()\n",
    "print(f\"HECHOS: {count_h} departamentos únicos\")\n",
    "deptos_hechos.orderBy(\"depto\").show(25, truncate=False)\n",
    "\n",
    "# Vehículos\n",
    "deptos_vehiculos = vehiculos_silver.select(\"depto\").filter(F.col(\"depto\").isNotNull()).distinct()\n",
    "count_v = deptos_vehiculos.count()\n",
    "print(f\"\\nVEHÍCULOS: {count_v} departamentos únicos\")\n",
    "deptos_vehiculos.orderBy(\"depto\").show(25, truncate=False)\n",
    "\n",
    "# Fallecidos\n",
    "deptos_fallecidos = fallecidos_silver.select(\"depto\").filter(F.col(\"depto\").isNotNull()).distinct()\n",
    "count_f = deptos_fallecidos.count()\n",
    "print(f\"\\nFALLECIDOS: {count_f} departamentos únicos\")\n",
    "deptos_fallecidos.orderBy(\"depto\").show(25, truncate=False)\n",
    "\n",
    "# BONUS: Departamentos que aparecen en las 3 bases\n",
    "deptos_comunes = (deptos_hechos\n",
    "    .intersect(deptos_vehiculos)\n",
    "    .intersect(deptos_fallecidos))\n",
    "print(f\"\\nDepartamentos comunes a las 3 bases: {deptos_comunes.count()}\")\n",
    "deptos_comunes.orderBy(\"depto\").show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2171a7-06ad-48f2-9f82-d8cb9bfed222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuál es el total de accidentes por año y departamento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeef8a38-0ce7-4c50-9b18-b80d3ce4c283",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"total_accidentes\":{\"format\":{\"preset\":\"number-preset-standard\",\"config\":{\"type\":\"number-plain\",\"abbreviation\":\"none\",\"decimalPlaces\":{\"type\":\"max\",\"places\":2},\"hideGroupSeparator\":false}}}}},\"syncTimestamp\":1759556178113}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGZfcDUgPSAoaGVjaG9zX3NpbHZlcgogICAgLmZpbHRlcihGLmNvbCgiYW5pbyIpLmlzTm90TnVsbCgpICYgRi5jb2woImRlcHRvIikuaXNOb3ROdWxsKCkpCiAgICAuZ3JvdXBCeSgiYW5pbyIsICJkZXB0byIpCiAgICAuY291bnQoKQogICAgLndpdGhDb2x1bW5SZW5hbWVkKCJjb3VudCIsICJ0b3RhbF9hY2NpZGVudGVzIikgICMg4oaQIFJlbm9tYnJhIGRlc3B1w6lzCiAgICAub3JkZXJCeSgiYW5pbyIsICJkZXB0byIpKQoKZGlzcGxheShkZl9wNSk=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView16b19db\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView16b19db\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView16b19db\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView16b19db) SELECT `depto`,SUM(`total_accidentes`) `column_ffd8cc2185`,`anio` FROM q GROUP BY `anio`,`depto`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView16b19db\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Chart",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "anio",
             "id": "column_b381ce0689"
            },
            "x": {
             "column": "depto",
             "id": "column_b381ce0685"
            },
            "y": [
             {
              "column": "total_accidentes",
              "id": "column_ffd8cc2185",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_b381ce0687": {
             "type": "column",
             "yAxis": 0
            },
            "column_ffd8cc2185": {
             "type": "column",
             "yAxis": 0,
             "zIndex": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "Total de accidentes"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "title": {
              "text": null
             },
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "59f45bf6-84f8-4074-9d33-0e3352389a51",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 23.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "depto",
           "type": "column"
          },
          {
           "column": "anio",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "depto",
           "type": "column"
          },
          {
           "alias": "column_ffd8cc2185",
           "args": [
            {
             "column": "total_accidentes",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "anio",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_p5 = (hechos_silver\n",
    "    .filter(F.col(\"anio\").isNotNull() & F.col(\"depto\").isNotNull())\n",
    "    .groupBy(\"anio\", \"depto\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"total_accidentes\")  # ← Renombra después\n",
    "    .orderBy(\"anio\", \"depto\"))\n",
    "\n",
    "display(df_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ab6450-a7ee-4094-8863-be30e1b1b996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. ¿Qué día de la semana registra más accidentes en 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622dc78f-b890-4e25-ac6b-202d7cd471cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBQcmVndW50YSA2CmRmX3A2ID0gKGhlY2hvc19zaWx2ZXIKICAgIC5maWx0ZXIoKEYuY29sKCJhbmlvIikgPT0gMjAyMykgJiBGLmNvbCgiZGlhX3NlbSIpLmlzTm90TnVsbCgpKQogICAgLmdyb3VwQnkoImRpYV9zZW0iKQogICAgLmNvdW50KCkgICMg4oaQIGVzdG8gY3JlYSBjb2x1bW5hICJjb3VudCIKICAgIC5vcmRlckJ5KEYuZGVzYygiY291bnQiKSkpCgpkaXNwbGF5KGRmX3A2KQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView7996a64\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView7996a64\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView7996a64\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView7996a64) SELECT `dia_sem`,SUM(`count`) `column_ad2b4ed0537` FROM q GROUP BY `dia_sem`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView7996a64\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "dia_sem",
             "id": "column_ad2b4ed0536"
            },
            "y": [
             {
              "column": "count",
              "id": "column_ad2b4ed0537",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_ad2b4ed0537": {
             "name": "count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "26db1732-f356-4f66-8545-4a03b6249538",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 26.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "dia_sem",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "dia_sem",
           "type": "column"
          },
          {
           "alias": "column_ad2b4ed0537",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pregunta 6\n",
    "df_p6 = (hechos_silver\n",
    "    .filter((F.col(\"anio\") == 2023) & F.col(\"dia_sem\").isNotNull())\n",
    "    .groupBy(\"dia_sem\")\n",
    "    .count()  # ← esto crea columna \"count\"\n",
    "    .orderBy(F.desc(\"count\")))\n",
    "\n",
    "display(df_p6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c770977e-9e5c-4b90-a373-9d611bb99672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Mostrar la distribución de accidentes por hora del día en el municipio de \n",
    "Guatemala. Graficar en un histograma.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7708c207-546b-444e-aecd-247804f1027f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"CmRmX3A3ID0gKGhlY2hvc19zaWx2ZXIKICAgIC5maWx0ZXIoCiAgICAgICAgRi5jb2woImhvcmEiKS5pc05vdE51bGwoKSAmIAogICAgICAgIChGLmNvbCgiZGVwdG8iKSA9PSAiR3VhdGVtYWxhIikgJgogICAgICAgIChGLmNvbCgibXVwaW8iKSA9PSAxKQogICAgKQogICAgLmdyb3VwQnkoImhvcmEiKQogICAgLmFnZyhGLmNvdW50KCIqIikuYWxpYXMoInRvdGFsX2FjY2lkZW50ZXMiKSkKICAgIC5vcmRlckJ5KCJob3JhIikpCgpkaXNwbGF5KGRmX3A3KQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView91ec227\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView91ec227\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView91ec227\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView91ec227) ,min_max AS (SELECT `hora`,(SELECT MAX(`hora`) FROM q) `target_column_max`,(SELECT MIN(`hora`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `hora`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 24 `step` FROM min_max) SELECT IF(ISNULL(`hora`),NULL,LEAST(WIDTH_BUCKET(`hora`,`min_value`,`max_value`,24),24)) `hora_BIN`,FIRST(`min_value` + ((IF(ISNULL(`hora`),NULL,LEAST(WIDTH_BUCKET(`hora`,`min_value`,`max_value`,24),24)) - 1) * `step`)) `hora_BIN_LOWER_BOUND`,FIRST(`step`) `hora_BIN_STEP`,COUNT(`hora`) `COUNT` FROM histogram_meta GROUP BY `hora_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView91ec227\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "hora",
             "id": "column_ad2b4ed0535"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 24,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "COUNT": {
             "color": "#077A9D"
            },
            "column_ad2b4ed0533": {
             "name": "total_accidentes",
             "type": "histogram",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "9fc6dc22-f3bc-4e0a-9a9e-0c6858875ddb",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 28.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "hora_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "hora_BIN",
           "args": [
            {
             "column": "hora",
             "type": "column"
            },
            {
             "number": 24,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "hora_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "hora",
             "type": "column"
            },
            {
             "number": 24,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "hora_BIN_STEP",
           "args": [
            {
             "column": "hora",
             "type": "column"
            },
            {
             "number": 24,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "hora",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_hist = (hechos_silver\n",
    "  .filter(F.col(\"hora\").isNotNull() & (F.col(\"depto\")==\"Guatemala\") & (F.col(\"mupio\")==1))\n",
    "  .select(F.col(\"hora\").cast(\"int\")))\n",
    "\n",
    "display(df_hist)  \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB 8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
