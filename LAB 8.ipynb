{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3698d1-880a-4c4d-be9a-e89d96d07651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Importar librerías y definir funciones base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cc9bc0-f6e4-4d60-ba7f-f8d6f9af28d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "%pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7c33f0-52c7-45a2-b49e-bc05babd962f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ff737ad-c5a3-4d8e-a16e-c2e01074fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carga desde archivo y construcción dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943e93f2-79e5-4c1d-a016-d46d687b0d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "\n",
    "directories = [\n",
    "    \"/Volumes/workspace/default/fallecidos_lesionados/\",\n",
    "    \"/Volumes/workspace/default/hechos_transito/\",\n",
    "    \"/Volumes/workspace/default/vehiculos_involucrados/\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    try:\n",
    "        files = [f.name for f in dbutils.fs.ls(directory)]\n",
    "        \n",
    "        for file in files:\n",
    "            lower_file = file.lower()\n",
    "            file_path = os.path.join(directory, file)\n",
    "            \n",
    "            # XLSX\n",
    "            if lower_file.endswith(\".xlsx\"):\n",
    "                csv_path = file_path.replace(\".xlsx\", \".csv\")\n",
    "                try:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    print(f\"Convertido XLSX → CSV: {file_path} → {csv_path}\")\n",
    "                    \n",
    "                    # Borrar original\n",
    "                    dbutils.fs.rm(file_path)\n",
    "                    print(f\"Borrado XLSX: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error con XLSX {file_path}: {e}\")\n",
    "            \n",
    "            # SAV\n",
    "            elif lower_file.endswith(\".sav\"):\n",
    "                csv_path = file_path.replace(\".sav\", \".csv\")\n",
    "                try:\n",
    "                    df, meta = pyreadstat.read_sav(file_path)\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    print(f\"Convertido SAV → CSV: {file_path} → {csv_path}\")\n",
    "                    \n",
    "                    # Borrar original\n",
    "                    dbutils.fs.rm(file_path)\n",
    "                    print(f\"Borrado SAV: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error con SAV {file_path}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar directorio {directory}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b15a6b2c-1cc0-4f2d-a320-fd46e3280130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unir TODOS los CSV de una carpeta en un solo DataFrame (robusto a columnas distintas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b0b017-f7cf-4256-802e-4a2d6e7f924a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "DIR_HECHOS = \"/Volumes/workspace/default/hechos_transito\"\n",
    "DIR_VEHICULOS = \"/Volumes/workspace/default/vehiculos_involucrados\"\n",
    "DIR_FALLECIDOS = \"/Volumes/workspace/default/fallecidos_lesionados\"\n",
    "\n",
    "# Opción simple (si todos los CSV ya tienen mismas columnas):\n",
    "hechos_raw = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DIR_HECHOS}/*.csv\")\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "display(hechos_raw.limit(5))\n",
    "hechos_raw.printSchema()\n",
    "\n",
    "vehiculos_raw = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DIR_VEHICULOS}/*.csv\")\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "display(vehiculos_raw.limit(5))\n",
    "vehiculos_raw.printSchema()\n",
    "\n",
    "fallecidos_raw = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"{DIR_FALLECIDOS}/*.csv\")\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    ")\n",
    "\n",
    "display(fallecidos_raw.limit(5))\n",
    "fallecidos_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5adc32-2e64-422a-9178-59d98711176b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# 1) alias de columnas → nombre canónico\n",
    "ALIASES = {\n",
    "    \"num_hecho\":      [\"num_hecho\", \"num\", \"num_correl\", \"num_corre\", \"n_um_corre\", \"num_correlativo\", \"núm_corre\"],\n",
    "    \"anio\":           [\"anio\", \"ano_ocu\", \"año_ocu\", \"anio_ocu\", \"ano\", \"año\"],\n",
    "    \"mes\":            [\"mes\", \"mes_ocu\"],\n",
    "    \"dia\":            [\"dia\", \"día\", \"dia_ocu\", \"día_ocu\"],\n",
    "    \"dia_sem\":        [\"dia_sem_ocu\", \"día_sem_ocu\", \"dia_sem\", \"día_sem\"],\n",
    "    \"hora\":           [\"hora\", \"hora_ocu\"],\n",
    "    \"g_hora\":         [\"g_hora\", \"g_hora_5\"],\n",
    "    \"depto\":          [\"depto_ocu\", \"depto\"],\n",
    "    \"mupio\":          [\"mupio_ocu\", \"muni_ocu\", \"municipio\", \"mupio\"],\n",
    "    \"zona\":           [\"zona_ocu\", \"zona\"],\n",
    "    \"area\":           [\"areag_ocu\", \"area_ocu\", \"area\"],\n",
    "    \"tipo_accidente\": [\"tipo_eve\", \"tipo_evento\", \"tipo\", \"tipo_acc\", \"tipo_accidente\"],\n",
    "    \"causa_acc\":      [\"causa_acc\", \"causa\"],\n",
    "    \"sexo_pil\":       [\"sexo_pil\", \"sexo_piloto\", \"sexo\", \"sexo_per\"],\n",
    "    \"edad_pil\":       [\"edad_pil\", \"edad\", \"edad_piloto\", \"edad_per\"],\n",
    "    \"g_edad\":         [\"g_edad_2\", \"g_edad\", \"g_edad_80ymás\", \"g_edad_60ymás\", \"edad_quinquenales\"],\n",
    "    \"mayor_menor\":    [\"mayor_menor\"],\n",
    "    \"tipo_veh\":       [\"tipo_veh\", \"tipo_vehiculo\"],\n",
    "    \"marca_veh\":      [\"marca_veh\", \"marca\"],\n",
    "    \"color_veh\":      [\"color_veh\", \"color\"],\n",
    "    \"modelo_veh\":     [\"modelo_veh\", \"modelo\"],\n",
    "    \"g_modelo_veh\":   [\"g_modelo_veh\"],\n",
    "    \"estado_pil\":     [\"estado_pil\", \"estado_piloto\", \"estado\", \"estado_con\", \"fall_les\"],\n",
    "    \"intencionalidad\":[\"int_o_noint\"],\n",
    "    \"source_file\":    [\"source_file\", \"_source_file\"]\n",
    "}\n",
    "\n",
    "def first_present(colnames, candidates):\n",
    "    cols_lower = {c.lower(): c for c in colnames}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def canonize(df):\n",
    "    # construir mapeo existente -> canon\n",
    "    mapping = {}\n",
    "    for canon, cands in ALIASES.items():\n",
    "        src = first_present(df.columns, cands)\n",
    "        if src and src != canon:\n",
    "            mapping[src] = canon\n",
    "\n",
    "    # aplicar renombres\n",
    "    for src, dst in mapping.items():\n",
    "        df = df.withColumnRenamed(src, dst)\n",
    "\n",
    "    return df\n",
    "\n",
    "hechos_std = canonize(hechos_raw)\n",
    "fallecidos_std = canonize(fallecidos_raw)\n",
    "vehiculos_std = canonize(vehiculos_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4fb1ec-49d9-4d76-b1df-59fe7a3e085e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Si no hay 'anio', lo extraemos del nombre del archivo\n",
    "if \"anio\" not in hechos_std.columns and \"source_file\" in hechos_std.columns:\n",
    "    hechos_std = hechos_std.withColumn(\n",
    "        \"anio\", F.regexp_extract(\"source_file\", r\"(20\\d{2})\", 1).cast(\"int\")\n",
    "    )\n",
    "\n",
    "if \"anio\" not in fallecidos_std.columns and \"source_file\" in fallecidos_std.columns:\n",
    "    fallecidos_std = fallecidos_std.withColumn(\n",
    "        \"anio\", F.regexp_extract(\"source_file\", r\"(20\\d{2})\", 1).cast(\"int\")\n",
    "    )\n",
    "\n",
    "if \"anio\" not in vehiculos_std.columns and \"source_file\" in vehiculos_std.columns:\n",
    "    vehiculos_std = vehiculos_std.withColumn(\n",
    "        \"anio\", F.regexp_extract(\"source_file\", r\"(20\\d{2})\", 1).cast(\"int\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58f84f7-e97f-474b-a8d0-5c95a2b4a8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "INT_COLS = [\"anio\",\"mes\",\"dia\",\"dia_sem\",\"hora\",\"g_hora\",\"depto\",\"mupio\",\"zona\",\"area\",\n",
    "            \"tipo_accidente\",\"causa_acc\",\"sexo_pil\",\"edad_pil\",\"g_edad\",\"mayor_menor\",\n",
    "            \"tipo_veh\",\"marca_veh\",\"color_veh\",\"modelo_veh\",\"g_modelo_veh\",\"estado_pil\"]\n",
    "\n",
    "# (1) Normaliza a string para inspección y limpia tokens comunes no-numéricos\n",
    "TOKENS_NULOS = [\"Ignorada\",\"IGNORADA\",\"NA\",\"N/A\",\"S/D\",\"SD\",\"Sin dato\",\"SIN DATO\",\"\", \" \"]\n",
    "\n",
    "dfs = [hechos_std, fallecidos_std, vehiculos_std]\n",
    "\n",
    "for df in dfs:\n",
    "    for c in INT_COLS:\n",
    "        if c in df.columns:\n",
    "            # todo como string primero\n",
    "            s = F.col(c).cast(\"string\")\n",
    "            # convierte a NULL si coincide con tokens nulos\n",
    "            s = F.when(F.lower(F.trim(s)).isin([t.lower() for t in TOKENS_NULOS]), None).otherwise(s)\n",
    "            # deja pasar solo enteros (± dígitos); si no, NULL\n",
    "            s = F.when(F.trim(s).rlike(r\"^-?\\d+$\"), s).otherwise(None)\n",
    "            # castea a int al final\n",
    "            df = df.withColumn(c, s.cast(T.IntegerType()))\n",
    "\n",
    "    # (2) regla específica: 9999 en modelo_veh -> NULL\n",
    "    if \"modelo_veh\" in df.columns:\n",
    "        df = df.withColumn(\"modelo_veh\", F.when(F.col(\"modelo_veh\")==9999, None).otherwise(F.col(\"modelo_veh\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e539c515-4a55-40bd-b665-376dce50d521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OUT_SILVER_HECHOS = \"/Volumes/workspace/default/hechos_transito/silver\"\n",
    "OUT_SILVER_VEHICULOS = \"/Volumes/workspace/default/vehiculos_involucrados/silver\"\n",
    "OUT_SILVER_FALLECIDOS = \"/Volumes/workspace/default/fallecidos_lesionados/silver\"\n",
    "\n",
    "dfs = [hechos_std, fallecidos_std, vehiculos_std]\n",
    "OUT_SILVERS = [OUT_SILVER_HECHOS, OUT_SILVER_VEHICULOS, OUT_SILVER_FALLECIDOS]\n",
    "\n",
    "for i in range(3):\n",
    "    df = dfs[i]\n",
    "    out_silver = OUT_SILVERS[i]\n",
    "    (df\n",
    "    .coalesce(1)\n",
    "    .write.mode(\"overwrite\")\n",
    "    .parquet(OUT_SILVER_HECHOS))\n",
    "\n",
    "    print(f\"{df} estandarizados →\", out_silver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "116815e8-ea38-4277-bae4-02473c7d9494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Helpers reutilizables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c84e375-2c6b-4029-a161-56ff5bdb174a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preguntas a responder\n",
    "1. Contar registros por tabla (long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54a41ebe-e411-4a1f-a68c-61496b1edbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## #1 – Conteos, .show(), describe y summary (por tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94f7ec7-3025-43d1-bd43-01b75c2eb39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ae_conteos_describe_summary(nombre: str, sdfs: dict, n_show: int = 5):\n",
    "    print(f\"\\n===== {nombre.upper()} :: Conteos por hoja =====\")\n",
    "    total_registros = 0\n",
    "    for key, sdf in sdfs.items():\n",
    "        c = sdf.count()\n",
    "        total_registros += c\n",
    "        print(f\"{key:20s} -> {c:6d} registros\")\n",
    "    print(f\"TOTAL {nombre}: {total_registros}\")\n",
    "\n",
    "    # Muestra de una hoja representativa (la primera)\n",
    "    if sdfs:\n",
    "        first_key = list(sdfs.keys())[0]\n",
    "        print(f\"\\n--- Ejemplo .show() :: {first_key} ---\")\n",
    "        sdfs[first_key].show(n_show, truncate=False)\n",
    "\n",
    "        # describe/summary de columnas numéricas\n",
    "        num_cols = [f.name for f in sdfs[first_key].schema.fields if isinstance(f.dataType, NumericType)]\n",
    "        if num_cols:\n",
    "            print(f\"\\n--- describe(numéricas) :: {first_key} ---\")\n",
    "            sdfs[first_key].select(*num_cols).describe().show(truncate=False)\n",
    "\n",
    "            print(f\"\\n--- summary(numéricas) :: {first_key} ---\")\n",
    "            sdfs[first_key].select(*num_cols).summary(\"count\",\"mean\",\"stddev\",\"min\",\"25%\",\"50%\",\"75%\",\"max\").show(truncate=False)\n",
    "        else:\n",
    "            print(f\"\\n--- {first_key}: no hay columnas numéricas detectadas ---\")\n",
    "\n",
    "ae_conteos_describe_summary(\"hechos\", hechos_sdfs)\n",
    "ae_conteos_describe_summary(\"vehiculos\", vehiculos_sdfs)\n",
    "ae_conteos_describe_summary(\"lesionados\", lesionados_sdfs)\n",
    "ae_conteos_describe_summary(\"fallecidos\", fallecidos_sdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e9f4429-aa75-46e1-916e-d5ce6ccb86da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## #2 – Años disponibles por tabla y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d30887e-f177-47ba-a250-9bdc965b0e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EXPECTED_YEARS = set(range(2020, 2025))  # 2020..2024\n",
    "YEAR_RX = re.compile(r'(?<!\\d)(20\\d{2})(?:\\.0)?(?!\\d)')\n",
    "\n",
    "def extract_year_from_col(colname: str) -> int | None:\n",
    "    m = YEAR_RX.search(str(colname))\n",
    "    if not m: return None\n",
    "    try:\n",
    "        return int(m.group(1))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def detect_years_in_pdf(df: pd.DataFrame) -> set[int]:\n",
    "    years = set()\n",
    "    for c in df.columns:\n",
    "        y = extract_year_from_col(c)\n",
    "        if y: years.add(y)\n",
    "    return years\n",
    "\n",
    "def report_years_dict(dict_pd: dict, titulo: str):\n",
    "    print(f\"\\n===== Verificación de años: {titulo} =====\")\n",
    "    all_years = set()\n",
    "    for key, df in dict_pd.items():\n",
    "        found = detect_years_in_pdf(df)\n",
    "        all_years |= found\n",
    "        missing = sorted(EXPECTED_YEARS - found)\n",
    "        outside = sorted(y for y in found if y not in EXPECTED_YEARS)\n",
    "        print(f\"[{key}] encontrados: {sorted(found) if found else '—'} | faltantes vs 2020–2024: {missing if missing else 'ninguno'} | fuera de rango: {outside if outside else 'ninguno'}\")\n",
    "    print(f\"\\nAños agregados (union) en {titulo}: {sorted(all_years) if all_years else '—'}\")\n",
    "    return all_years\n",
    "\n",
    "years_hechos     = report_years_dict(hechos_dfs, \"hechos\")\n",
    "years_vehiculos  = report_years_dict(vehiculos_dfs, \"vehiculos\")\n",
    "years_lesionados = report_years_dict(lesionados_dfs, \"lesionados\")\n",
    "years_fallecidos = report_years_dict(fallecidos_dfs, \"fallecidos\")\n",
    "\n",
    "print(\"\\n¿Coinciden los conjuntos de años (intersección)?\")\n",
    "intersection_all = years_hechos & years_vehiculos & years_lesionados & years_fallecidos\n",
    "print(\"Intersección común:\", sorted(intersection_all) if intersection_all else \"—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c520ac04-b689-4c5e-a959-7aafa3d4abd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### #3 – Valores distintos de 'tipo de accidente'   (buscamos columnas candidatas por nombre aproximado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7248bd6-9365-4bcf-8328-b1848c56981d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Accidente (largo + ancho) → tidy, robusto ===\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\n",
    "\n",
    "# --- normalizadores ---\n",
    "def _norm_text(s):\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")  # quita acentos\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _norm_colname(s):\n",
    "    if s is None: return \"\"\n",
    "    t = str(s).strip().lower()\n",
    "    t = (t.replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\")\n",
    "           .replace(\"ó\",\"o\").replace(\"ú\",\"u\").replace(\"ñ\",\"n\"))\n",
    "    t = re.sub(r\"[\\s\\-]+\", \"_\", t)\n",
    "    return t\n",
    "\n",
    "NOTE_PAT = re.compile(r\"(fuente|nota|serie|el mes que no aparece|no se presentaron|sin datos)\", re.IGNORECASE)\n",
    "\n",
    "def _drop_noise_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    # descarta filas de notas en cualquier columna\n",
    "    mask_any = df.apply(lambda r: r.astype(str).str.contains(NOTE_PAT, na=False).any(), axis=1)\n",
    "    return df.loc[~mask_any].copy()\n",
    "\n",
    "# --- catálogo canónico y mapping ---\n",
    "CANON = [\"colision\",\"Atropello\",\"Derrape\",\"Choque\",\"Vuelco\",\"Embarrancó\",\"Encunetó\",\"Caída\",\"Ignorado\"]\n",
    "MAP_NORM_TO_CANON = {\n",
    "    \"colision\":\"colision\",\"colisiones\":\"colision\",\"colision multiple\":\"colision\",\n",
    "    \"atropello\":\"Atropello\",\"atropellos\":\"Atropello\",\n",
    "    \"derrape\":\"Derrape\",\"derrapes\":\"Derrape\",\n",
    "    \"choque\":\"Choque\",\"choques\":\"Choque\",\n",
    "    \"vuelco\":\"Vuelco\",\"vuelcos\":\"Vuelco\",\n",
    "    \"embarranco\":\"Embarrancó\",\"embarranco multiple\":\"Embarrancó\",\n",
    "    \"encuneto\":\"Encunetó\",\"encunetamiento\":\"Encunetó\",\n",
    "    \"caida\":\"Caída\",\"caidas\":\"Caída\",\n",
    "    \"ignorado\":\"Ignorado\",\"desconocido\":\"Ignorado\",\"no especificado\":\"Ignorado\",\"sin dato\":\"Ignorado\",\"na\":\"Ignorado\",\"n/a\":\"Ignorado\",\n",
    "}\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def canon_val_udf(val):\n",
    "    if val is None: return None\n",
    "    n = _norm_text(val)\n",
    "    if not n: return None\n",
    "    return MAP_NORM_TO_CANON.get(n, \"Ignorado\")\n",
    "\n",
    "def _find_long_tipo_col(df: pd.DataFrame) -> str | None:\n",
    "    # busca “tipo_de_accidente” (normalizado) o abreviado “tipo_de_accid”\n",
    "    for c in df.columns:\n",
    "        nc = _norm_colname(c)\n",
    "        if nc == \"tipo_de_accidente\" or nc.startswith(\"tipo_de_accid\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _find_wide_cols(df: pd.DataFrame):\n",
    "    \"\"\"Devuelve (cols, rename) para columnas de accidentes en formato ANCHO.\"\"\"\n",
    "    cols, ren = [], {}\n",
    "    for c in df.columns:\n",
    "        n = _norm_text(c)\n",
    "        if n in MAP_NORM_TO_CANON:\n",
    "            canon = MAP_NORM_TO_CANON[n]\n",
    "            # acepta solo si parece conteo (al menos un número en la col)\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if s.notna().any():\n",
    "                cols.append(c)\n",
    "                ren[c] = canon\n",
    "    return cols, ren\n",
    "\n",
    "def _build_tidy_from_long(df: pd.DataFrame, sheet_name: str):\n",
    "    # espera columna \"tipo de accidente\" + columnas de conteos (años/meses)\n",
    "    tipo_col = _find_long_tipo_col(df)\n",
    "    if not tipo_col: \n",
    "        return None  # no es formato largo\n",
    "    work = _drop_noise_rows(df)\n",
    "    # columnas numéricas candidatas (conteos)\n",
    "    num_cols = []\n",
    "    for c in work.columns:\n",
    "        if c == tipo_col: continue\n",
    "        if pd.to_numeric(work[c], errors=\"coerce\").notna().mean() > 0.3:\n",
    "            num_cols.append(c)\n",
    "    if not num_cols: \n",
    "        return None\n",
    "    # sumar por tipo (excluye filas 'total' en cualquier col de texto)\n",
    "    mask_total = work.apply(lambda r: r.astype(str).str.strip().str.lower().eq('total').any(), axis=1)\n",
    "    work = work.loc[~mask_total].copy()\n",
    "    work[\"__tipo__\"] = work[tipo_col].map(lambda x: MAP_NORM_TO_CANON.get(_norm_text(x), \"Ignorado\"))\n",
    "    sums = work[num_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).groupby(work[\"__tipo__\"]).sum().sum(axis=1)\n",
    "    tidy_pd = sums.reset_index().rename(columns={\"__tipo__\":\"accidente\",0:\"count\"})\n",
    "    tidy_pd[\"sheet\"] = sheet_name\n",
    "    return tidy_pd[[\"sheet\",\"accidente\",\"count\"]]\n",
    "\n",
    "def _build_tidy_from_wide(df: pd.DataFrame, sheet_name: str):\n",
    "    cols, ren = _find_wide_cols(df)\n",
    "    if not cols: \n",
    "        return None\n",
    "    work = _drop_noise_rows(df)\n",
    "    pdf = work[cols].rename(columns=ren).copy()\n",
    "    for c in pdf.columns:\n",
    "        pdf[c] = pd.to_numeric(pdf[c], errors=\"coerce\")\n",
    "    # Si hay filas “Total” en alguna col de texto ajena, igual sumar todo (salvo si quieres excluir explícitamente)\n",
    "    sums = {canon: float(pdf.get(canon, pd.Series(dtype=float)).fillna(0).sum()) for canon in CANON}\n",
    "    tidy_pd = pd.DataFrame({\"sheet\": [sheet_name]*len(CANON),\n",
    "                            \"accidente\": CANON,\n",
    "                            \"count\": [sums[k] for k in CANON]})\n",
    "    return tidy_pd\n",
    "\n",
    "def accidentes_tidy_from_dict(dict_pd: dict, titulo: str):\n",
    "    \"\"\"Devuelve un Spark DF tidy con columnas: sheet, accidente, count, combinando hojas largo/ancho.\"\"\"\n",
    "    rows = []\n",
    "    for key, df in dict_pd.items():\n",
    "        t_long = _build_tidy_from_long(df, key)\n",
    "        if t_long is not None:\n",
    "            rows.append(t_long)\n",
    "            continue\n",
    "        t_wide = _build_tidy_from_wide(df, key)\n",
    "        if t_wide is not None:\n",
    "            rows.append(t_wide)\n",
    "    if not rows:\n",
    "        print(f\"[{titulo}] No encontré info de accidentes en este conjunto.\")\n",
    "        return spark.createDataFrame([], schema=StructType([\n",
    "            StructField(\"sheet\", StringType(), True),\n",
    "            StructField(\"accidente\", StringType(), True),\n",
    "            StructField(\"count\", DoubleType(), True),\n",
    "        ]))\n",
    "    tidy_all = pd.concat(rows, ignore_index=True)\n",
    "    sdf = spark.createDataFrame(tidy_all.astype({\"sheet\":\"string\",\"accidente\":\"string\",\"count\":\"float\"}))\n",
    "    print(f\"[{titulo}] {sdf.count()} filas tidy (sheet, accidente, count)\")\n",
    "    return sdf\n",
    "\n",
    "# === USO ===\n",
    "acc_hechos_sdf     = accidentes_tidy_from_dict(hechos_dfs, \"hechos\")\n",
    "acc_vehiculos_sdf  = accidentes_tidy_from_dict(vehiculos_dfs, \"vehiculos\")\n",
    "acc_lesionados_sdf = accidentes_tidy_from_dict(lesionados_dfs, \"lesionados\")\n",
    "acc_fallecidos_sdf = accidentes_tidy_from_dict(fallecidos_dfs, \"fallecidos\")\n",
    "\n",
    "# Ejemplos de consulta:\n",
    "print(\"\\nTop 10 (hechos) por count:\")\n",
    "acc_hechos_sdf.groupBy(\"accidente\").agg(F.sum(\"count\").alias(\"total\")).orderBy(F.desc(\"total\")).show(10, truncate=False)\n",
    "\n",
    "print(\"\\nHojas que sólo tuvieron 'Ignorado' (vehículos):\")\n",
    "(acc_vehiculos_sdf\n",
    " .groupBy(\"sheet\")\n",
    " .pivot(\"accidente\")\n",
    " .agg(F.sum(\"count\"))\n",
    " .where(F.col(\"Ignorado\").isNotNull() &\n",
    "        sum(F.col(c).isNull() | (F.col(c) == 0) for c in CANON if c != \"Ignorado\") == (len(CANON)-1))\n",
    " .select(\"sheet\",\"Ignorado\")\n",
    " .show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fa2a743-5189-4fd7-b0b4-162b44d8c37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### #4 – # de departamentos únicos por base(detecta columna 'departamento' aproximada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55136fb-cc2f-4978-b86e-4e65d5f956dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_departamentos_unique(dict_sdfs: dict, titulo: str,\n",
    "                               dep_cands=(\"departamento\",\"departamentos\",\"depto\",\"dept\",\"depart\",\"depar\")):\n",
    "    print(f\"\\n===== Departamentos únicos :: {titulo} =====\")\n",
    "    deptos = set()\n",
    "    cols_encontradas = 0\n",
    "\n",
    "    # Aux: si no tienes ya definida find_first_column, deja esto aquí\n",
    "    def find_first_column(candidates: list[str], columns: list[str]) -> str | None:\n",
    "        for col in columns:\n",
    "            low = col.lower()\n",
    "            for cand in candidates:\n",
    "                if cand in low:  # match por substring\n",
    "                    return col\n",
    "        return None\n",
    "\n",
    "    for key, sdf in dict_sdfs.items():\n",
    "        col = find_first_column(list(dep_cands), sdf.columns)\n",
    "        if not col:\n",
    "            continue\n",
    "        cols_encontradas += 1\n",
    "\n",
    "        vals = (sdf\n",
    "                .select(F.col(col).cast(\"string\").alias(\"departamento\"))\n",
    "                .where(F.col(\"departamento\").isNotNull() & (F.col(\"departamento\") != \"\"))\n",
    "                .distinct()\n",
    "                .toPandas()[\"departamento\"])\n",
    "\n",
    "        for v in vals:\n",
    "            if v is not None:\n",
    "                deptos.add(str(v).strip())\n",
    "\n",
    "    print(f\"Total únicos (unión de hojas): {len(deptos)}\")\n",
    "    print(f\"Columnas 'departamento' detectadas en {cols_encontradas} hojas\")\n",
    "    if deptos:\n",
    "        print(\"Ejemplos:\", sorted(list(deptos))[:15])\n",
    "count_departamentos_unique(hechos_sdfs, \"hechos\")\n",
    "count_departamentos_unique(vehiculos_sdfs, \"vehiculos\")\n",
    "count_departamentos_unique(lesionados_sdfs, \"lesionados\")\n",
    "count_departamentos_unique(fallecidos_sdfs, \"fallecidos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd2171a7-06ad-48f2-9f82-d8cb9bfed222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuál es el total de accidentes por año y departamento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01ab6450-a7ee-4094-8863-be30e1b1b996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. ¿Qué día de la semana registra más accidentes en 2024?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB 8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
