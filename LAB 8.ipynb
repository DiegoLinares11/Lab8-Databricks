{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3698d1-880a-4c4d-be9a-e89d96d07651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Importar librerías y definir funciones base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cc9bc0-f6e4-4d60-ba7f-f8d6f9af28d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "%pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7c33f0-52c7-45a2-b49e-bc05babd962f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61efac13-cedb-4a8b-b954-2f3915a46890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ===========================\n",
    "# Lab CC3066 – Spark DataFrames (INE/PNC 2020–2024)\n",
    "# Ruta de trabajo del Excel (Databricks Volumes)\n",
    "EXCEL_PATH = \"/Volumes/workspace/default/pncc/20250527162011hk9xzLjtlLyIqA5fF0FY3udjjRUQlTkq.xlsx\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, NumericType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.appName(\"Lab_CC3066\").getOrCreate()\n",
    "\n",
    "# ==============\n",
    "# 1) Utilidades\n",
    "# ==============\n",
    "\n",
    "HEADER_HINTS = [\n",
    "    \"departamento\", \"vehículo\", \"vehiculo\", \"mes de ocurrencia\", \"día de la semana\",\n",
    "    \"día de ocurrencia\", \"hora de ocurrencia\", \"condición del conductor y sexo\",\n",
    "    \"grupos de edad\", \"tipo de vehículo y sexo de la persona\", \"zona de ocurrencia\",\n",
    "    \"tipo de accidente y sexo\", \"tipo\", \"clase\", \"marca\", \"modelo de vehículo\",\n",
    "    \"sexo\", \"grupo\", \"edad\", \"tipo de vehículo\", \"clases de vehículos\", \"color de vehículo\"\n",
    "]\n",
    "\n",
    "def safe_col(name: str) -> str:\n",
    "    n = str(name).strip()\n",
    "    n = re.sub(r'\\.0$', '', n)              # 2021.0 -> 2021\n",
    "    n = n.replace('%', 'pct')\n",
    "    n = re.sub(r'\\s+', '_', n)              # espacios -> _\n",
    "    n = re.sub(r'[^0-9a-zA-Z_]', '_', n)    # símbolos raros -> _\n",
    "    if re.match(r'^\\d', n):                 # si inicia con dígito, prefijo\n",
    "        n = f'y{n}'\n",
    "    return n\n",
    "\n",
    "def make_unique(cols):\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        base = (c or \"col\")\n",
    "        if base not in seen:\n",
    "            seen[base] = 0\n",
    "            out.append(base)\n",
    "        else:\n",
    "            seen[base] += 1\n",
    "            out.append(f\"{base}_{seen[base]}\")\n",
    "    return out\n",
    "\n",
    "def to_scalar(x):\n",
    "    # Convierte contenedores a texto; deja escalares tal cual\n",
    "    if isinstance(x, (list, dict, tuple, set)):\n",
    "        return str(x) if len(x) else None\n",
    "    return x\n",
    "\n",
    "# Detectar fila de encabezados buscando palabras clave\n",
    "def find_header_row(raw: pd.DataFrame) -> int | None:\n",
    "    for i in range(len(raw)):\n",
    "        row = raw.iloc[i].astype(str).str.lower()\n",
    "        if any(word in row.tolist() for word in HEADER_HINTS):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "NOTE_PAT = re.compile(r\"(fuente|nota|serie|el mes que no aparece|no se presentaron|sin datos)\", re.IGNORECASE)\n",
    "\n",
    "def drop_noise_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # descarta si la PRIMERA columna parece nota\n",
    "    mask1 = df.iloc[:,0].astype(str).str.contains(NOTE_PAT, na=False)\n",
    "    # y descarta filas donde CUALQUIER columna tenga frases típicas de nota\n",
    "    mask_any = df.apply(lambda r: r.astype(str).str.contains(NOTE_PAT, na=False).any(), axis=1)\n",
    "    return df.loc[~(mask1 | mask_any)].copy()\n",
    "\n",
    "def read_cuadro(excel_path: str, sheet_num: int, categoria: str) -> pd.DataFrame | None:\n",
    "    sheet = f\"cuadro {sheet_num}\"\n",
    "    raw = pd.read_excel(excel_path, sheet_name=sheet, header=None)\n",
    "\n",
    "    header_idx = find_header_row(raw)\n",
    "    if header_idx is None or header_idx + 1 >= len(raw):\n",
    "        return None\n",
    "\n",
    "    header_row = raw.iloc[header_idx].tolist()\n",
    "    next_row   = raw.iloc[header_idx + 1].tolist()\n",
    "\n",
    "    combined_header = []\n",
    "    prefer_detail = {\"año de ocurrencia\",\"mes de ocurrencia\",\"tipo de accidente\",\"día de la semana\",\n",
    "                     \"total\",\"hombre\",\"mujer\",\"ignorado\",\"grupos de edad\"}\n",
    "    for h, n in zip(header_row, next_row):\n",
    "        if pd.isna(h) or str(h).strip() == \"\":\n",
    "            combined_header.append(n)\n",
    "        elif str(h).strip().lower() in prefer_detail:\n",
    "            combined_header.append(n)  # usa detalle de la segunda fila\n",
    "        else:\n",
    "            combined_header.append(h)\n",
    "\n",
    "    df = raw.iloc[header_idx + 2:].copy()\n",
    "    df = df.dropna(how=\"all\")\n",
    "    df = drop_noise_rows(df)\n",
    "\n",
    "    # Remueve notas/fuentes, etc.\n",
    "    df = df[~df.iloc[:,0].astype(str).str.contains(\"fuente|nota|cuadro|serie\", case=False, na=False)]\n",
    "    df.columns = [str(c).strip() for c in combined_header]\n",
    "    df[\"categoria\"] = categoria\n",
    "    df[\"cuadro\"] = sheet_num\n",
    "\n",
    "    # Filas con primera columna vacía fuera\n",
    "    df = df[df.iloc[:,0].notna() & (df.iloc[:,0].astype(str).str.strip() != \"\")]\n",
    "    # Quita columnas completamente vacías\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    return df\n",
    "\n",
    "def build_dict(excel_path: str, rango: range, categoria: str) -> dict[str, pd.DataFrame]:\n",
    "    x = {}\n",
    "    for i in rango:\n",
    "        try:\n",
    "            df = read_cuadro(excel_path, i, categoria)\n",
    "            if df is not None:\n",
    "                x[f\"{categoria}_{i}\"] = df\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error leyendo {categoria} cuadro {i}: {e}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ff737ad-c5a3-4d8e-a16e-c2e01074fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carga desde archivo y construcción dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943e93f2-79e5-4c1d-a016-d46d687b0d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "directories = [\n",
    "    \"/Volumes/workspace/default/fallecidos_lesionados/\",\n",
    "    \"/Volumes/workspace/default/hechos_transito/\",\n",
    "    \"/Volumes/workspace/default/vehiculos_involucrados/\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    try:\n",
    "        files = [f.name for f in dbutils.fs.ls(directory)]\n",
    "        \n",
    "        for file in files:\n",
    "            lower_file = file.lower()\n",
    "            file_path = os.path.join(directory, file)\n",
    "            \n",
    "            # XLSX\n",
    "            if lower_file.endswith(\".xlsx\"):\n",
    "                csv_path = file_path.replace(\".xlsx\", \".csv\")\n",
    "                try:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    print(f\"Convertido XLSX → CSV: {file_path} → {csv_path}\")\n",
    "                    \n",
    "                    # Borrar original\n",
    "                    dbutils.fs.rm(file_path)\n",
    "                    print(f\"Borrado XLSX: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error con XLSX {file_path}: {e}\")\n",
    "            \n",
    "            # SAV\n",
    "            elif lower_file.endswith(\".sav\"):\n",
    "                csv_path = file_path.replace(\".sav\", \".csv\")\n",
    "                try:\n",
    "                    df, meta = pyreadstat.read_sav(file_path)\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    print(f\"Convertido SAV → CSV: {file_path} → {csv_path}\")\n",
    "                    \n",
    "                    # Borrar original\n",
    "                    dbutils.fs.rm(file_path)\n",
    "                    print(f\"Borrado SAV: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error con SAV {file_path}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar directorio {directory}: {e}\")\n",
    "\n",
    "\n",
    "hechos_dfs     = build_dict(EXCEL_PATH, range(1, 17),  \"hechos\")\n",
    "vehiculos_dfs  = build_dict(EXCEL_PATH, range(17, 29), \"vehiculos\")\n",
    "lesionados_dfs = build_dict(EXCEL_PATH, range(31, 47), \"lesionados\")\n",
    "fallecidos_dfs = build_dict(EXCEL_PATH, range(47, 63), \"fallecidos\")\n",
    "\n",
    "print(\"Hechos cargados    :\", list(hechos_dfs.keys())[:5], \"...\")\n",
    "print(\"Vehículos cargados :\", list(vehiculos_dfs.keys())[:5], \"...\")\n",
    "print(\"Lesionados cargados:\", list(lesionados_dfs.keys())[:5], \"...\")\n",
    "print(\"Fallecidos cargados:\", list(fallecidos_dfs.keys())[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d13c0a1-dd11-442c-84a2-873baae1f646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Función: pandas DF -> Spark DF con tipado robusto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47db670a-63c3-4b27-909d-baa135c0db7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pandas_to_spark(df_pd: pd.DataFrame):\n",
    "    # 1) elimina filas 'total' en cualquier columna\n",
    "    mask_total = df_pd.apply(lambda r: r.astype(str).str.strip().str.lower().eq('total').any(), axis=1)\n",
    "    dfc = df_pd.loc[~mask_total].copy()\n",
    "\n",
    "    # 2) normaliza nombres y unicidad\n",
    "    dfc.columns = [safe_col(c) for c in dfc.columns]\n",
    "    dfc = dfc.dropna(axis=1, how='all')\n",
    "    dfc.columns = make_unique(dfc.columns)\n",
    "\n",
    "    # 3) contenedores -> cadena/None\n",
    "    dfc = dfc.applymap(to_scalar)\n",
    "\n",
    "    # 4) tipificación 80%\n",
    "    col_is_numeric = {}\n",
    "    for c in dfc.columns:\n",
    "        ser = pd.to_numeric(dfc[c], errors=\"coerce\")\n",
    "        ratio = ser.notna().mean() if len(ser) else 0.0\n",
    "        if ratio >= 0.8:\n",
    "            dfc[c] = ser.astype(float)\n",
    "            col_is_numeric[c] = True\n",
    "        else:\n",
    "            dfc[c] = dfc[c].where(pd.notnull(dfc[c]), None).astype(\"string\")\n",
    "            col_is_numeric[c] = False\n",
    "\n",
    "    # 5) NaN -> None\n",
    "    dfc = dfc.where(pd.notnull(dfc), None)\n",
    "\n",
    "    # 6) schema explícito\n",
    "    schema = StructType([\n",
    "        StructField(c, DoubleType() if col_is_numeric.get(c, False) else StringType(), True)\n",
    "        for c in dfc.columns\n",
    "    ])\n",
    "\n",
    "    sdf = spark.createDataFrame(dfc.astype(object), schema=schema)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed9eb318-da6a-47db-b057-f28b48cc6a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Construir Spark DFs por categoría (dicts de SDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc033a5-b194-4b15-abb9-01712dcbd5c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dict_pandas_to_spark(dict_pandas: dict) -> dict:\n",
    "    out = {}\n",
    "    for key, df in dict_pandas.items():\n",
    "        out[key] = pandas_to_spark(df)\n",
    "    return out\n",
    "\n",
    "hechos_sdfs     = dict_pandas_to_spark(hechos_dfs)\n",
    "vehiculos_sdfs  = dict_pandas_to_spark(vehiculos_dfs)\n",
    "lesionados_sdfs = dict_pandas_to_spark(lesionados_dfs)\n",
    "fallecidos_sdfs = dict_pandas_to_spark(fallecidos_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb099cb-bc64-4106-b0a7-eccf2968169a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def choose_tipo_vehiculo_col(columns: list[str]) -> str | None:\n",
    "    # busca \"tipo\" o \"clase\" + \"vehicul\"\n",
    "    cands = [c for c in columns if (\"vehicul\" in c.lower()) and (\"tipo\" in c.lower() or \"clase\" in c.lower())]\n",
    "    if not cands:\n",
    "        # plan B: columnas con nombres conocidos\n",
    "        cands = [c for c in columns if c.lower() in {\"tipo_de_vehículo\",\"tipo_de_vehiculo\",\"clases_de_vehículos\",\"clases_de_vehiculos\"}]\n",
    "    return cands[0] if cands else None\n",
    "\n",
    "def distinct_tipo_vehiculo(dict_sdfs: dict, titulo: str, limit=200):\n",
    "    print(f\"\\n===== Distinct 'tipo de vehículo' :: {titulo} =====\")\n",
    "    seen = set()\n",
    "    cols_elegidas = []\n",
    "    for key, sdf in dict_sdfs.items():\n",
    "        col = choose_tipo_vehiculo_col(sdf.columns)\n",
    "        if not col:\n",
    "            continue\n",
    "        if not is_mostly_string(sdf, col):\n",
    "            continue\n",
    "        cols_elegidas.append((key, col))\n",
    "        vals = (sdf\n",
    "                .select(F.col(col).cast(\"string\").alias(\"tipo_vehiculo\"))\n",
    "                .where(F.col(\"tipo_vehiculo\").isNotNull() & (F.col(\"tipo_vehiculo\") != \"\"))\n",
    "                .distinct()\n",
    "                .limit(limit)\n",
    "                .toPandas()[\"tipo_vehiculo\"])\n",
    "        for v in vals:\n",
    "            seen.add(v.strip())\n",
    "    if not cols_elegidas:\n",
    "        print(\"No se encontró columna elegible para 'tipo de vehículo'.\")\n",
    "        return\n",
    "    print(\"Columnas elegidas:\", cols_elegidas[:6], \"...\" if len(cols_elegidas)>6 else \"\")\n",
    "    print(f\"{len(seen)} valores distintos:\")\n",
    "    for v in sorted(seen):\n",
    "        print(\"-\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c84e375-2c6b-4029-a161-56ff5bdb174a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preguntas a responder\n",
    "1. Contar registros por tabla (long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54a41ebe-e411-4a1f-a68c-61496b1edbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## #1 – Conteos, .show(), describe y summary (por tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94f7ec7-3025-43d1-bd43-01b75c2eb39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ae_conteos_describe_summary(nombre: str, sdfs: dict, n_show: int = 5):\n",
    "    print(f\"\\n===== {nombre.upper()} :: Conteos por hoja =====\")\n",
    "    total_registros = 0\n",
    "    for key, sdf in sdfs.items():\n",
    "        c = sdf.count()\n",
    "        total_registros += c\n",
    "        print(f\"{key:20s} -> {c:6d} registros\")\n",
    "    print(f\"TOTAL {nombre}: {total_registros}\")\n",
    "\n",
    "    # Muestra de una hoja representativa (la primera)\n",
    "    if sdfs:\n",
    "        first_key = list(sdfs.keys())[0]\n",
    "        print(f\"\\n--- Ejemplo .show() :: {first_key} ---\")\n",
    "        sdfs[first_key].show(n_show, truncate=False)\n",
    "\n",
    "        # describe/summary de columnas numéricas\n",
    "        num_cols = [f.name for f in sdfs[first_key].schema.fields if isinstance(f.dataType, NumericType)]\n",
    "        if num_cols:\n",
    "            print(f\"\\n--- describe(numéricas) :: {first_key} ---\")\n",
    "            sdfs[first_key].select(*num_cols).describe().show(truncate=False)\n",
    "\n",
    "            print(f\"\\n--- summary(numéricas) :: {first_key} ---\")\n",
    "            sdfs[first_key].select(*num_cols).summary(\"count\",\"mean\",\"stddev\",\"min\",\"25%\",\"50%\",\"75%\",\"max\").show(truncate=False)\n",
    "        else:\n",
    "            print(f\"\\n--- {first_key}: no hay columnas numéricas detectadas ---\")\n",
    "\n",
    "ae_conteos_describe_summary(\"hechos\", hechos_sdfs)\n",
    "ae_conteos_describe_summary(\"vehiculos\", vehiculos_sdfs)\n",
    "ae_conteos_describe_summary(\"lesionados\", lesionados_sdfs)\n",
    "ae_conteos_describe_summary(\"fallecidos\", fallecidos_sdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e9f4429-aa75-46e1-916e-d5ce6ccb86da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## #2 – Años disponibles por tabla y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d30887e-f177-47ba-a250-9bdc965b0e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EXPECTED_YEARS = set(range(2020, 2025))  # 2020..2024\n",
    "YEAR_RX = re.compile(r'(?<!\\d)(20\\d{2})(?:\\.0)?(?!\\d)')\n",
    "\n",
    "def extract_year_from_col(colname: str) -> int | None:\n",
    "    m = YEAR_RX.search(str(colname))\n",
    "    if not m: return None\n",
    "    try:\n",
    "        return int(m.group(1))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def detect_years_in_pdf(df: pd.DataFrame) -> set[int]:\n",
    "    years = set()\n",
    "    for c in df.columns:\n",
    "        y = extract_year_from_col(c)\n",
    "        if y: years.add(y)\n",
    "    return years\n",
    "\n",
    "def report_years_dict(dict_pd: dict, titulo: str):\n",
    "    print(f\"\\n===== Verificación de años: {titulo} =====\")\n",
    "    all_years = set()\n",
    "    for key, df in dict_pd.items():\n",
    "        found = detect_years_in_pdf(df)\n",
    "        all_years |= found\n",
    "        missing = sorted(EXPECTED_YEARS - found)\n",
    "        outside = sorted(y for y in found if y not in EXPECTED_YEARS)\n",
    "        print(f\"[{key}] encontrados: {sorted(found) if found else '—'} | faltantes vs 2020–2024: {missing if missing else 'ninguno'} | fuera de rango: {outside if outside else 'ninguno'}\")\n",
    "    print(f\"\\nAños agregados (union) en {titulo}: {sorted(all_years) if all_years else '—'}\")\n",
    "    return all_years\n",
    "\n",
    "years_hechos     = report_years_dict(hechos_dfs, \"hechos\")\n",
    "years_vehiculos  = report_years_dict(vehiculos_dfs, \"vehiculos\")\n",
    "years_lesionados = report_years_dict(lesionados_dfs, \"lesionados\")\n",
    "years_fallecidos = report_years_dict(fallecidos_dfs, \"fallecidos\")\n",
    "\n",
    "print(\"\\n¿Coinciden los conjuntos de años (intersección)?\")\n",
    "intersection_all = years_hechos & years_vehiculos & years_lesionados & years_fallecidos\n",
    "print(\"Intersección común:\", sorted(intersection_all) if intersection_all else \"—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c520ac04-b689-4c5e-a959-7aafa3d4abd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### #3 – Valores distintos de 'tipo de accidente'   (buscamos columnas candidatas por nombre aproximado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7248bd6-9365-4bcf-8328-b1848c56981d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "def choose_tipo_accidente_col(columns: list[str]) -> str | None:\n",
    "    cols_lower = {c.lower(): c for c in columns}\n",
    "    # 1) candidatos con \"tipo\" y \"accid\"\n",
    "    cands = [c for c in columns if (\"tipo\" in c.lower() and \"accid\" in c.lower())]\n",
    "    # 2) excluir \"vehicul\"\n",
    "    cands = [c for c in cands if \"vehicul\" not in c.lower()]\n",
    "    # 3) fallback suave: algo que suene a clasificación de accidente\n",
    "    if not cands:\n",
    "        cands = [c for c in columns if (\"clasific\" in c.lower() and \"accid\" in c.lower())\n",
    "                 and \"vehicul\" not in c.lower()]\n",
    "    if not cands:\n",
    "        return None\n",
    "    return cands[0]\n",
    "\n",
    "def is_mostly_string(sdf, colname, sample=200, string_ratio=0.6):\n",
    "    # toma una muestra y estima si la mayoría son valores no numéricos y no vacíos\n",
    "    pdf = (sdf\n",
    "           .select(F.col(colname).cast(\"string\").alias(\"v\"))\n",
    "           .where(F.col(\"v\").isNotNull() & (F.col(\"v\") != \"\"))\n",
    "           .limit(sample)\n",
    "           .toPandas())\n",
    "    if pdf.empty:\n",
    "        return False\n",
    "    ser = pdf[\"v\"]\n",
    "    # no-numérico si to_numeric da NaN\n",
    "    non_num = pd.to_numeric(ser, errors=\"coerce\").isna().mean()\n",
    "    return non_num >= string_ratio\n",
    "\n",
    "def distinct_tipo_accidente(dict_sdfs: dict, titulo: str, limit=200):\n",
    "    print(f\"\\n===== Distinct 'tipo de accidente' :: {titulo} =====\")\n",
    "    seen = set()\n",
    "    cols_elegidas = []\n",
    "    for key, sdf in dict_sdfs.items():\n",
    "        col = choose_tipo_accidente_col(sdf.columns)\n",
    "        if not col:\n",
    "            continue\n",
    "        # evita columnas numéricas mal detectadas\n",
    "        if not is_mostly_string(sdf, col):\n",
    "            continue\n",
    "        cols_elegidas.append((key, col))\n",
    "        vals = (sdf\n",
    "                .select(F.col(col).cast(\"string\").alias(\"tipo_accidente\"))\n",
    "                .where(F.col(\"tipo_accidente\").isNotNull() & (F.col(\"tipo_accidente\") != \"\"))\n",
    "                .distinct()\n",
    "                .limit(limit)\n",
    "                .toPandas()[\"tipo_accidente\"])\n",
    "        for v in vals:\n",
    "            seen.add(v.strip())\n",
    "    if not cols_elegidas:\n",
    "        print(\"No se encontró columna elegible para 'tipo de accidente' en estas hojas.\")\n",
    "        return\n",
    "    print(\"Columnas elegidas:\", cols_elegidas[:6], \"...\" if len(cols_elegidas)>6 else \"\")\n",
    "    print(f\"{len(seen)} valores distintos:\")\n",
    "    for v in sorted(seen):\n",
    "        print(\"-\", v)\n",
    "\n",
    "\n",
    "distinct_tipo_accidente(hechos_sdfs, \"hechos\")\n",
    "distinct_tipo_accidente(vehiculos_sdfs, \"vehiculos\")\n",
    "distinct_tipo_accidente(lesionados_sdfs, \"lesionados\")\n",
    "distinct_tipo_accidente(fallecidos_sdfs, \"fallecidos\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fa2a743-5189-4fd7-b0b4-162b44d8c37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### #4 – # de departamentos únicos por base(detecta columna 'departamento' aproximada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55136fb-cc2f-4978-b86e-4e65d5f956dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_departamentos_unique(dict_sdfs: dict, titulo: str,\n",
    "                               dep_cands=(\"departamento\",\"departamentos\",\"depto\",\"dept\",\"depart\",\"depar\")):\n",
    "    print(f\"\\n===== Departamentos únicos :: {titulo} =====\")\n",
    "    deptos = set()\n",
    "    cols_encontradas = 0\n",
    "\n",
    "    # Aux: si no tienes ya definida find_first_column, deja esto aquí\n",
    "    def find_first_column(candidates: list[str], columns: list[str]) -> str | None:\n",
    "        for col in columns:\n",
    "            low = col.lower()\n",
    "            for cand in candidates:\n",
    "                if cand in low:  # match por substring\n",
    "                    return col\n",
    "        return None\n",
    "\n",
    "    for key, sdf in dict_sdfs.items():\n",
    "        col = find_first_column(list(dep_cands), sdf.columns)\n",
    "        if not col:\n",
    "            continue\n",
    "        cols_encontradas += 1\n",
    "\n",
    "        vals = (sdf\n",
    "                .select(F.col(col).cast(\"string\").alias(\"departamento\"))\n",
    "                .where(F.col(\"departamento\").isNotNull() & (F.col(\"departamento\") != \"\"))\n",
    "                .distinct()\n",
    "                .toPandas()[\"departamento\"])\n",
    "\n",
    "        for v in vals:\n",
    "            if v is not None:\n",
    "                deptos.add(str(v).strip())\n",
    "\n",
    "    print(f\"Total únicos (unión de hojas): {len(deptos)}\")\n",
    "    print(f\"Columnas 'departamento' detectadas en {cols_encontradas} hojas\")\n",
    "    if deptos:\n",
    "        print(\"Ejemplos:\", sorted(list(deptos))[:15])\n",
    "count_departamentos_unique(hechos_sdfs, \"hechos\")\n",
    "count_departamentos_unique(vehiculos_sdfs, \"vehiculos\")\n",
    "count_departamentos_unique(lesionados_sdfs, \"lesionados\")\n",
    "count_departamentos_unique(fallecidos_sdfs, \"fallecidos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd2171a7-06ad-48f2-9f82-d8cb9bfed222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuál es el total de accidentes por año y departamento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01ab6450-a7ee-4094-8863-be30e1b1b996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. ¿Qué día de la semana registra más accidentes en 2024?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB 8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
